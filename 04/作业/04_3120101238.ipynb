{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict (before training) 4 4.0\n",
      "Epoch: 0 w= 1.0933333333333333 loss= 4.666666666666667\n",
      "Epoch: 1 w= 1.1779555555555554 loss= 3.8362074074074086\n",
      "Epoch: 2 w= 1.2546797037037036 loss= 3.1535329869958857\n",
      "Epoch: 3 w= 1.3242429313580246 loss= 2.592344272332262\n",
      "Epoch: 4 w= 1.3873135910979424 loss= 2.1310222071581117\n",
      "Epoch: 5 w= 1.4444976559288012 loss= 1.7517949663820642\n",
      "Epoch: 6 w= 1.4963445413754464 loss= 1.440053319920117\n",
      "Epoch: 7 w= 1.5433523841804047 loss= 1.1837878313441108\n",
      "Epoch: 8 w= 1.5859728283235668 loss= 0.9731262101573632\n",
      "Epoch: 9 w= 1.6246153643467005 loss= 0.7999529948031382\n",
      "Epoch: 10 w= 1.659651263674342 loss= 0.6575969151946154\n",
      "Epoch: 11 w= 1.6914171457314033 loss= 0.5405738908195378\n",
      "Epoch: 12 w= 1.7202182121298057 loss= 0.44437576375991855\n",
      "Epoch: 13 w= 1.7463311789976905 loss= 0.365296627844598\n",
      "Epoch: 14 w= 1.7700069356245727 loss= 0.3002900634939416\n",
      "Epoch: 15 w= 1.7914729549662791 loss= 0.2468517784170642\n",
      "Epoch: 16 w= 1.8109354791694263 loss= 0.2029231330489788\n",
      "Epoch: 17 w= 1.8285815011136133 loss= 0.16681183417217407\n",
      "Epoch: 18 w= 1.8445805610096762 loss= 0.1371267415488235\n",
      "Epoch: 19 w= 1.8590863753154396 loss= 0.11272427607497944\n",
      "Predict(after training) 4 7.4363455012617585\n",
      "predict (before training) 4 4.0\n",
      "\tgrad: 1.0 2.0 -2.0\n",
      "\tgrad: 2.0 4.0 -7.84\n",
      "\tgrad: 3.0 6.0 -16.2288\n",
      "progress: 0 w= 1.260688 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.478624\n",
      "\tgrad: 2.0 4.0 -5.796206079999999\n",
      "\tgrad: 3.0 6.0 -11.998146585599997\n",
      "progress: 1 w= 1.453417766656 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.093164466688\n",
      "\tgrad: 2.0 4.0 -4.285204709416961\n",
      "\tgrad: 3.0 6.0 -8.87037374849311\n",
      "progress: 2 w= 1.5959051959019805 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.8081896081960389\n",
      "\tgrad: 2.0 4.0 -3.1681032641284723\n",
      "\tgrad: 3.0 6.0 -6.557973756745939\n",
      "progress: 3 w= 1.701247862192685 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.59750427561463\n",
      "\tgrad: 2.0 4.0 -2.3422167604093502\n",
      "\tgrad: 3.0 6.0 -4.848388694047353\n",
      "progress: 4 w= 1.7791289594933983 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.44174208101320334\n",
      "\tgrad: 2.0 4.0 -1.7316289575717576\n",
      "\tgrad: 3.0 6.0 -3.584471942173538\n",
      "progress: 5 w= 1.836707389300983 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.3265852213980338\n",
      "\tgrad: 2.0 4.0 -1.2802140678802925\n",
      "\tgrad: 3.0 6.0 -2.650043120512205\n",
      "progress: 6 w= 1.8792758133988885 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.241448373202223\n",
      "\tgrad: 2.0 4.0 -0.946477622952715\n",
      "\tgrad: 3.0 6.0 -1.9592086795121197\n",
      "progress: 7 w= 1.910747160155559 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.17850567968888198\n",
      "\tgrad: 2.0 4.0 -0.6997422643804168\n",
      "\tgrad: 3.0 6.0 -1.4484664872674653\n",
      "progress: 8 w= 1.9340143044689266 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.13197139106214673\n",
      "\tgrad: 2.0 4.0 -0.5173278529636143\n",
      "\tgrad: 3.0 6.0 -1.0708686556346834\n",
      "progress: 9 w= 1.9512159834655312 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.09756803306893769\n",
      "\tgrad: 2.0 4.0 -0.38246668963023644\n",
      "\tgrad: 3.0 6.0 -0.7917060475345892\n",
      "progress: 10 w= 1.9639333911678687 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.07213321766426262\n",
      "\tgrad: 2.0 4.0 -0.2827622132439096\n",
      "\tgrad: 3.0 6.0 -0.5853177814148953\n",
      "progress: 11 w= 1.9733355232910992 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.05332895341780164\n",
      "\tgrad: 2.0 4.0 -0.2090494973977819\n",
      "\tgrad: 3.0 6.0 -0.4327324596134101\n",
      "progress: 12 w= 1.9802866323953892 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.039426735209221686\n",
      "\tgrad: 2.0 4.0 -0.15455280202014876\n",
      "\tgrad: 3.0 6.0 -0.3199243001817109\n",
      "progress: 13 w= 1.9854256707695 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.02914865846100012\n",
      "\tgrad: 2.0 4.0 -0.11426274116712065\n",
      "\tgrad: 3.0 6.0 -0.2365238742159388\n",
      "progress: 14 w= 1.9892250235079405 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.021549952984118992\n",
      "\tgrad: 2.0 4.0 -0.08447581569774698\n",
      "\tgrad: 3.0 6.0 -0.17486493849433593\n",
      "progress: 15 w= 1.9920339305797026 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.015932138840594856\n",
      "\tgrad: 2.0 4.0 -0.062453984255132156\n",
      "\tgrad: 3.0 6.0 -0.12927974740812687\n",
      "progress: 16 w= 1.994110589284741 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.011778821430517894\n",
      "\tgrad: 2.0 4.0 -0.046172980007630926\n",
      "\tgrad: 3.0 6.0 -0.09557806861579543\n",
      "progress: 17 w= 1.9956458879852805 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.008708224029438938\n",
      "\tgrad: 2.0 4.0 -0.03413623819540135\n",
      "\tgrad: 3.0 6.0 -0.07066201306448505\n",
      "progress: 18 w= 1.9967809527381737 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.006438094523652627\n",
      "\tgrad: 2.0 4.0 -0.02523733053271826\n",
      "\tgrad: 3.0 6.0 -0.052241274202728505\n",
      "progress: 19 w= 1.9976201197307648 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.004759760538470381\n",
      "\tgrad: 2.0 4.0 -0.01865826131080439\n",
      "\tgrad: 3.0 6.0 -0.03862260091336722\n",
      "progress: 20 w= 1.998240525958391 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.0035189480832178432\n",
      "\tgrad: 2.0 4.0 -0.01379427648621423\n",
      "\tgrad: 3.0 6.0 -0.028554152326460525\n",
      "progress: 21 w= 1.99869919972735 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.002601600545300009\n",
      "\tgrad: 2.0 4.0 -0.01019827413757568\n",
      "\tgrad: 3.0 6.0 -0.021110427464781978\n",
      "progress: 22 w= 1.9990383027488265 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.001923394502346909\n",
      "\tgrad: 2.0 4.0 -0.007539706449199102\n",
      "\tgrad: 3.0 6.0 -0.01560719234984198\n",
      "progress: 23 w= 1.9992890056818404 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.0014219886363191492\n",
      "\tgrad: 2.0 4.0 -0.005574195454370212\n",
      "\tgrad: 3.0 6.0 -0.011538584590544687\n",
      "progress: 24 w= 1.999474353368653 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.0010512932626940419\n",
      "\tgrad: 2.0 4.0 -0.004121069589761106\n",
      "\tgrad: 3.0 6.0 -0.008530614050808794\n",
      "progress: 25 w= 1.9996113831376856 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.0007772337246287897\n",
      "\tgrad: 2.0 4.0 -0.0030467562005451754\n",
      "\tgrad: 3.0 6.0 -0.006306785335127074\n",
      "progress: 26 w= 1.9997126908902887 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.0005746182194226179\n",
      "\tgrad: 2.0 4.0 -0.002252503420136165\n",
      "\tgrad: 3.0 6.0 -0.00466268207967957\n",
      "progress: 27 w= 1.9997875889274812 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.0004248221450375844\n",
      "\tgrad: 2.0 4.0 -0.0016653028085471533\n",
      "\tgrad: 3.0 6.0 -0.0034471768136938863\n",
      "progress: 28 w= 1.9998429619451539 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.00031407610969225175\n",
      "\tgrad: 2.0 4.0 -0.0012311783499932005\n",
      "\tgrad: 3.0 6.0 -0.0025485391844828342\n",
      "progress: 29 w= 1.9998838998815958 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.00023220023680847746\n",
      "\tgrad: 2.0 4.0 -0.0009102249282886277\n",
      "\tgrad: 3.0 6.0 -0.0018841656015560204\n",
      "progress: 30 w= 1.9999141657892625 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.00017166842147497974\n",
      "\tgrad: 2.0 4.0 -0.0006729402121816719\n",
      "\tgrad: 3.0 6.0 -0.0013929862392156878\n",
      "progress: 31 w= 1.9999365417379913 loss= 1\n",
      "\tgrad: 1.0 2.0 -0.0001269165240174175\n",
      "\tgrad: 2.0 4.0 -0.0004975127741477792\n",
      "\tgrad: 3.0 6.0 -0.0010298514424817995\n",
      "progress: 32 w= 1.9999530845453979 loss= 1\n",
      "\tgrad: 1.0 2.0 -9.383090920422887e-05\n",
      "\tgrad: 2.0 4.0 -0.00036781716408107457\n",
      "\tgrad: 3.0 6.0 -0.0007613815296476645\n",
      "progress: 33 w= 1.9999653148414271 loss= 1\n",
      "\tgrad: 1.0 2.0 -6.937031714571162e-05\n",
      "\tgrad: 2.0 4.0 -0.0002719316432120422\n",
      "\tgrad: 3.0 6.0 -0.0005628985014531906\n",
      "progress: 34 w= 1.999974356846045 loss= 1\n",
      "\tgrad: 1.0 2.0 -5.1286307909848006e-05\n",
      "\tgrad: 2.0 4.0 -0.00020104232700646207\n",
      "\tgrad: 3.0 6.0 -0.0004161576169003922\n",
      "progress: 35 w= 1.9999810417085633 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.7916582873442906e-05\n",
      "\tgrad: 2.0 4.0 -0.0001486330048638962\n",
      "\tgrad: 3.0 6.0 -0.0003076703200690645\n",
      "progress: 36 w= 1.9999859839076413 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.8032184717474706e-05\n",
      "\tgrad: 2.0 4.0 -0.0001098861640933535\n",
      "\tgrad: 3.0 6.0 -0.00022746435967313516\n",
      "progress: 37 w= 1.9999896377347262 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.0724530547688857e-05\n",
      "\tgrad: 2.0 4.0 -8.124015974608767e-05\n",
      "\tgrad: 3.0 6.0 -0.00016816713067413502\n",
      "progress: 38 w= 1.999992339052936 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.5321894128117464e-05\n",
      "\tgrad: 2.0 4.0 -6.006182498197177e-05\n",
      "\tgrad: 3.0 6.0 -0.00012432797771566584\n",
      "progress: 39 w= 1.9999943361699042 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.1327660191629008e-05\n",
      "\tgrad: 2.0 4.0 -4.4404427951505454e-05\n",
      "\tgrad: 3.0 6.0 -9.191716585732479e-05\n",
      "progress: 40 w= 1.9999958126624442 loss= 1\n",
      "\tgrad: 1.0 2.0 -8.37467511161094e-06\n",
      "\tgrad: 2.0 4.0 -3.282872643772805e-05\n",
      "\tgrad: 3.0 6.0 -6.795546372551087e-05\n",
      "progress: 41 w= 1.999996904251097 loss= 1\n",
      "\tgrad: 1.0 2.0 -6.191497806007362e-06\n",
      "\tgrad: 2.0 4.0 -2.4270671399762023e-05\n",
      "\tgrad: 3.0 6.0 -5.0240289795056015e-05\n",
      "progress: 42 w= 1.999997711275687 loss= 1\n",
      "\tgrad: 1.0 2.0 -4.5774486259198e-06\n",
      "\tgrad: 2.0 4.0 -1.794359861406747e-05\n",
      "\tgrad: 3.0 6.0 -3.714324913239864e-05\n",
      "progress: 43 w= 1.9999983079186507 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.3841626985164908e-06\n",
      "\tgrad: 2.0 4.0 -1.326591777761621e-05\n",
      "\tgrad: 3.0 6.0 -2.7460449796734565e-05\n",
      "progress: 44 w= 1.9999987490239537 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.5019520926150562e-06\n",
      "\tgrad: 2.0 4.0 -9.807652203264183e-06\n",
      "\tgrad: 3.0 6.0 -2.0301840059744336e-05\n",
      "progress: 45 w= 1.9999990751383971 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.8497232057157476e-06\n",
      "\tgrad: 2.0 4.0 -7.250914967116273e-06\n",
      "\tgrad: 3.0 6.0 -1.5009393983689279e-05\n",
      "progress: 46 w= 1.9999993162387186 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.3675225627451937e-06\n",
      "\tgrad: 2.0 4.0 -5.3606884460322135e-06\n",
      "\tgrad: 3.0 6.0 -1.109662508014253e-05\n",
      "progress: 47 w= 1.9999994944870796 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.0110258408246864e-06\n",
      "\tgrad: 2.0 4.0 -3.963221296032771e-06\n",
      "\tgrad: 3.0 6.0 -8.20386808086937e-06\n",
      "progress: 48 w= 1.9999996262682318 loss= 1\n",
      "\tgrad: 1.0 2.0 -7.474635363990956e-07\n",
      "\tgrad: 2.0 4.0 -2.930057062755509e-06\n",
      "\tgrad: 3.0 6.0 -6.065218119744031e-06\n",
      "progress: 49 w= 1.999999723695619 loss= 1\n",
      "\tgrad: 1.0 2.0 -5.526087618612507e-07\n",
      "\tgrad: 2.0 4.0 -2.166226346744793e-06\n",
      "\tgrad: 3.0 6.0 -4.484088535150477e-06\n",
      "progress: 50 w= 1.9999997957248556 loss= 1\n",
      "\tgrad: 1.0 2.0 -4.08550288710785e-07\n",
      "\tgrad: 2.0 4.0 -1.6015171322436572e-06\n",
      "\tgrad: 3.0 6.0 -3.3151404608133817e-06\n",
      "progress: 51 w= 1.9999998489769344 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.020461312175371e-07\n",
      "\tgrad: 2.0 4.0 -1.1840208351543424e-06\n",
      "\tgrad: 3.0 6.0 -2.4509231284497446e-06\n",
      "progress: 52 w= 1.9999998883468353 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.2330632942768602e-07\n",
      "\tgrad: 2.0 4.0 -8.753608113920563e-07\n",
      "\tgrad: 3.0 6.0 -1.811996877876254e-06\n",
      "progress: 53 w= 1.9999999174534755 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.6509304900935717e-07\n",
      "\tgrad: 2.0 4.0 -6.471647520100987e-07\n",
      "\tgrad: 3.0 6.0 -1.3396310407642886e-06\n",
      "progress: 54 w= 1.999999938972364 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.220552721115098e-07\n",
      "\tgrad: 2.0 4.0 -4.784566662863199e-07\n",
      "\tgrad: 3.0 6.0 -9.904052991061008e-07\n",
      "progress: 55 w= 1.9999999548815364 loss= 1\n",
      "\tgrad: 1.0 2.0 -9.023692726373156e-08\n",
      "\tgrad: 2.0 4.0 -3.5372875473171916e-07\n",
      "\tgrad: 3.0 6.0 -7.322185204827747e-07\n",
      "progress: 56 w= 1.9999999666433785 loss= 1\n",
      "\tgrad: 1.0 2.0 -6.671324292994996e-08\n",
      "\tgrad: 2.0 4.0 -2.615159129248923e-07\n",
      "\tgrad: 3.0 6.0 -5.413379398078177e-07\n",
      "progress: 57 w= 1.9999999753390494 loss= 1\n",
      "\tgrad: 1.0 2.0 -4.932190122985958e-08\n",
      "\tgrad: 2.0 4.0 -1.9334185274999527e-07\n",
      "\tgrad: 3.0 6.0 -4.002176350326181e-07\n",
      "progress: 58 w= 1.9999999817678633 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.6464273378555845e-08\n",
      "\tgrad: 2.0 4.0 -1.429399514307761e-07\n",
      "\tgrad: 3.0 6.0 -2.9588569994132286e-07\n",
      "progress: 59 w= 1.9999999865207625 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.6958475007887728e-08\n",
      "\tgrad: 2.0 4.0 -1.0567722164012139e-07\n",
      "\tgrad: 3.0 6.0 -2.1875184863517916e-07\n",
      "progress: 60 w= 1.999999990034638 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.993072418216002e-08\n",
      "\tgrad: 2.0 4.0 -7.812843882959442e-08\n",
      "\tgrad: 3.0 6.0 -1.617258700292723e-07\n",
      "progress: 61 w= 1.9999999926324883 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.473502342363986e-08\n",
      "\tgrad: 2.0 4.0 -5.7761292637792394e-08\n",
      "\tgrad: 3.0 6.0 -1.195658771990793e-07\n",
      "progress: 62 w= 1.99999999455311 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.0893780100218464e-08\n",
      "\tgrad: 2.0 4.0 -4.270361841918202e-08\n",
      "\tgrad: 3.0 6.0 -8.839649012770678e-08\n",
      "progress: 63 w= 1.9999999959730488 loss= 1\n",
      "\tgrad: 1.0 2.0 -8.05390243385773e-09\n",
      "\tgrad: 2.0 4.0 -3.1571296688071016e-08\n",
      "\tgrad: 3.0 6.0 -6.53525820126788e-08\n",
      "progress: 64 w= 1.9999999970228268 loss= 1\n",
      "\tgrad: 1.0 2.0 -5.9543463493128e-09\n",
      "\tgrad: 2.0 4.0 -2.334103754719763e-08\n",
      "\tgrad: 3.0 6.0 -4.8315948575350376e-08\n",
      "progress: 65 w= 1.9999999977989402 loss= 1\n",
      "\tgrad: 1.0 2.0 -4.402119557767037e-09\n",
      "\tgrad: 2.0 4.0 -1.725630838222969e-08\n",
      "\tgrad: 3.0 6.0 -3.5720557178819945e-08\n",
      "progress: 66 w= 1.9999999983727301 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.254539748809293e-09\n",
      "\tgrad: 2.0 4.0 -1.2757796596929438e-08\n",
      "\tgrad: 3.0 6.0 -2.6408640607655798e-08\n",
      "progress: 67 w= 1.9999999987969397 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.406120636067044e-09\n",
      "\tgrad: 2.0 4.0 -9.431992964437086e-09\n",
      "\tgrad: 3.0 6.0 -1.9524227568012975e-08\n",
      "progress: 68 w= 1.999999999110563 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.7788739370416806e-09\n",
      "\tgrad: 2.0 4.0 -6.97318647269185e-09\n",
      "\tgrad: 3.0 6.0 -1.4434496264925656e-08\n",
      "progress: 69 w= 1.9999999993424284 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.3151431055291596e-09\n",
      "\tgrad: 2.0 4.0 -5.155360582875801e-09\n",
      "\tgrad: 3.0 6.0 -1.067159693945996e-08\n",
      "progress: 70 w= 1.9999999995138495 loss= 1\n",
      "\tgrad: 1.0 2.0 -9.72300906454393e-10\n",
      "\tgrad: 2.0 4.0 -3.811418736177075e-09\n",
      "\tgrad: 3.0 6.0 -7.88963561149103e-09\n",
      "progress: 71 w= 1.9999999996405833 loss= 1\n",
      "\tgrad: 1.0 2.0 -7.18833437218791e-10\n",
      "\tgrad: 2.0 4.0 -2.8178277489132597e-09\n",
      "\tgrad: 3.0 6.0 -5.832902161273523e-09\n",
      "progress: 72 w= 1.999999999734279 loss= 1\n",
      "\tgrad: 1.0 2.0 -5.314420015167798e-10\n",
      "\tgrad: 2.0 4.0 -2.0832526814729135e-09\n",
      "\tgrad: 3.0 6.0 -4.31233715403323e-09\n",
      "progress: 73 w= 1.9999999998035491 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.92901711165905e-10\n",
      "\tgrad: 2.0 4.0 -1.5401742103904326e-09\n",
      "\tgrad: 3.0 6.0 -3.188159070077745e-09\n",
      "progress: 74 w= 1.9999999998547615 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.9047697580608656e-10\n",
      "\tgrad: 2.0 4.0 -1.1386696030513122e-09\n",
      "\tgrad: 3.0 6.0 -2.3570478902001923e-09\n",
      "progress: 75 w= 1.9999999998926234 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.1475310418850313e-10\n",
      "\tgrad: 2.0 4.0 -8.418314934033333e-10\n",
      "\tgrad: 3.0 6.0 -1.7425900722400911e-09\n",
      "progress: 76 w= 1.9999999999206153 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.5876944203796484e-10\n",
      "\tgrad: 2.0 4.0 -6.223768167501476e-10\n",
      "\tgrad: 3.0 6.0 -1.2883241140571045e-09\n",
      "progress: 77 w= 1.9999999999413098 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.17380327679939e-10\n",
      "\tgrad: 2.0 4.0 -4.601314884666863e-10\n",
      "\tgrad: 3.0 6.0 -9.524754318590567e-10\n",
      "progress: 78 w= 1.9999999999566096 loss= 1\n",
      "\tgrad: 1.0 2.0 -8.678080476443029e-11\n",
      "\tgrad: 2.0 4.0 -3.4018121652934497e-10\n",
      "\tgrad: 3.0 6.0 -7.041780492045291e-10\n",
      "progress: 79 w= 1.9999999999679208 loss= 1\n",
      "\tgrad: 1.0 2.0 -6.415845632545825e-11\n",
      "\tgrad: 2.0 4.0 -2.5150193039280566e-10\n",
      "\tgrad: 3.0 6.0 -5.206075570640678e-10\n",
      "progress: 80 w= 1.9999999999762834 loss= 1\n",
      "\tgrad: 1.0 2.0 -4.743316850408519e-11\n",
      "\tgrad: 2.0 4.0 -1.8593837580738182e-10\n",
      "\tgrad: 3.0 6.0 -3.8489211817704927e-10\n",
      "progress: 81 w= 1.999999999982466 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.5067948545020045e-11\n",
      "\tgrad: 2.0 4.0 -1.3746692673066718e-10\n",
      "\tgrad: 3.0 6.0 -2.845563784603655e-10\n",
      "progress: 82 w= 1.9999999999870368 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.5926372160256506e-11\n",
      "\tgrad: 2.0 4.0 -1.0163070385260653e-10\n",
      "\tgrad: 3.0 6.0 -2.1037571684701106e-10\n",
      "progress: 83 w= 1.999999999990416 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.9167778475548403e-11\n",
      "\tgrad: 2.0 4.0 -7.51381179497912e-11\n",
      "\tgrad: 3.0 6.0 -1.5553425214420713e-10\n",
      "progress: 84 w= 1.9999999999929146 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.4170886686315498e-11\n",
      "\tgrad: 2.0 4.0 -5.555023108172463e-11\n",
      "\tgrad: 3.0 6.0 -1.1499068364173581e-10\n",
      "progress: 85 w= 1.9999999999947617 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.0476508549572827e-11\n",
      "\tgrad: 2.0 4.0 -4.106759377009439e-11\n",
      "\tgrad: 3.0 6.0 -8.500933290633839e-11\n",
      "progress: 86 w= 1.9999999999961273 loss= 1\n",
      "\tgrad: 1.0 2.0 -7.745359908994942e-12\n",
      "\tgrad: 2.0 4.0 -3.036149109902908e-11\n",
      "\tgrad: 3.0 6.0 -6.285105769165966e-11\n",
      "progress: 87 w= 1.999999999997137 loss= 1\n",
      "\tgrad: 1.0 2.0 -5.726086271806707e-12\n",
      "\tgrad: 2.0 4.0 -2.2446045022661565e-11\n",
      "\tgrad: 3.0 6.0 -4.646416584819235e-11\n",
      "progress: 88 w= 1.9999999999978835 loss= 1\n",
      "\tgrad: 1.0 2.0 -4.233058348290797e-12\n",
      "\tgrad: 2.0 4.0 -1.659294923683774e-11\n",
      "\tgrad: 3.0 6.0 -3.4351188560322043e-11\n",
      "progress: 89 w= 1.9999999999984353 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.1294966618133913e-12\n",
      "\tgrad: 2.0 4.0 -1.226752033289813e-11\n",
      "\tgrad: 3.0 6.0 -2.539835008974478e-11\n",
      "progress: 90 w= 1.9999999999988431 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.3137047833188262e-12\n",
      "\tgrad: 2.0 4.0 -9.070078021977679e-12\n",
      "\tgrad: 3.0 6.0 -1.8779644506139448e-11\n",
      "progress: 91 w= 1.9999999999991447 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.7106316363424412e-12\n",
      "\tgrad: 2.0 4.0 -6.7057470687359455e-12\n",
      "\tgrad: 3.0 6.0 -1.3882228699912957e-11\n",
      "progress: 92 w= 1.9999999999993676 loss= 1\n",
      "\tgrad: 1.0 2.0 -1.2647660696529783e-12\n",
      "\tgrad: 2.0 4.0 -4.957811938766099e-12\n",
      "\tgrad: 3.0 6.0 -1.0263789818054647e-11\n",
      "progress: 93 w= 1.9999999999995324 loss= 1\n",
      "\tgrad: 1.0 2.0 -9.352518759442319e-13\n",
      "\tgrad: 2.0 4.0 -3.666400516522117e-12\n",
      "\tgrad: 3.0 6.0 -7.58859641791787e-12\n",
      "progress: 94 w= 1.9999999999996543 loss= 1\n",
      "\tgrad: 1.0 2.0 -6.914468997365475e-13\n",
      "\tgrad: 2.0 4.0 -2.7107205369247822e-12\n",
      "\tgrad: 3.0 6.0 -5.611511255665391e-12\n",
      "progress: 95 w= 1.9999999999997444 loss= 1\n",
      "\tgrad: 1.0 2.0 -5.111466805374221e-13\n",
      "\tgrad: 2.0 4.0 -2.0037305148434825e-12\n",
      "\tgrad: 3.0 6.0 -4.1460168631601846e-12\n",
      "progress: 96 w= 1.999999999999811 loss= 1\n",
      "\tgrad: 1.0 2.0 -3.779199175824033e-13\n",
      "\tgrad: 2.0 4.0 -1.4814816040598089e-12\n",
      "\tgrad: 3.0 6.0 -3.064215547965432e-12\n",
      "progress: 97 w= 1.9999999999998603 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.793321129956894e-13\n",
      "\tgrad: 2.0 4.0 -1.0942358130705543e-12\n",
      "\tgrad: 3.0 6.0 -2.2648549702353193e-12\n",
      "progress: 98 w= 1.9999999999998967 loss= 1\n",
      "\tgrad: 1.0 2.0 -2.0650148258027912e-13\n",
      "\tgrad: 2.0 4.0 -8.100187187665142e-13\n",
      "\tgrad: 3.0 6.0 -1.6786572132332367e-12\n",
      "progress: 99 w= 1.9999999999999236 loss= 1\n",
      "Predict (after training) 4 7.9999999999996945\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x_data=[1.0,2.0,3.0]\n",
    "y_data=[2.0,4.0,6.0]\n",
    "w=1.0\n",
    "def forword(x):\n",
    "    return w*x\n",
    "def cost(xs,ys):\n",
    "    cost=0\n",
    "    for x,y in zip(xs,ys):#zip()将xs，ys的值一一对应生成坐标点\n",
    "        y_pred=forword(x)\n",
    "        cost+=(y_pred-y)**2\n",
    "    return cost/len(xs)\n",
    "def gradient(xs,ys):\n",
    "    grad=0\n",
    "    for x,y in zip(xs,ys):\n",
    "        grad+=2*x*(w*x-y)# 对损失函数求导，要使损失函数最小\n",
    "    return grad/len(xs)\n",
    "print('Predict (before training)',4,forword(4))\n",
    "loss=[]\n",
    "for epoch in range(20):\n",
    "    cost_val=cost(x_data,y_data)\n",
    "    grad_val=gradient(x_data,y_data)\n",
    "    w-=0.01*grad_val\n",
    "    loss.append(cost_val)\n",
    "    print('Epoch:',epoch,'w=',w,'loss=',cost_val)\n",
    "print('Predict(after training)',4,forword(4))\n",
    "plt.plot(loss)\n",
    "w=1.0\n",
    "def forward(x):\n",
    "    return w*x\n",
    "def loss(x,y):\n",
    "    y_pred=forward(x)\n",
    "    return (y_pred-y)**2\n",
    "def gradient(x,y):\n",
    "    return 2*x*(w*x-y)\n",
    "print('predict (before training)',4,forward(4))\n",
    "loss_=[]\n",
    "for epoch in range(100):\n",
    "    for x,y in zip(x_data,y_data):\n",
    "        grad=gradient(x,y)\n",
    "        w=w-0.01*grad\n",
    "        print(\"\\tgrad:\",x,y,grad)\n",
    "        l=loss(x,y)\n",
    "        loss_.append(l)\n",
    "    print('progress:',epoch,'w=',w,'loss=',1)\n",
    "print('Predict (after training)',4,forward(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad: 1.0 2.0 -2.0\n",
      "\tgrad: 2.0 4.0 -7.840000152587891\n",
      "\tgrad: 3.0 6.0 -16.228801727294922\n",
      "progress: 0 7.315943717956543\n",
      "predict (after training): 4 5.042752265930176\n",
      "\tgrad: 1.0 2.0 -1.478623867034912\n",
      "\tgrad: 2.0 4.0 -5.796205520629883\n",
      "\tgrad: 3.0 6.0 -11.998146057128906\n",
      "progress: 1 3.9987640380859375\n",
      "predict (after training): 4 5.813671112060547\n",
      "\tgrad: 1.0 2.0 -1.0931644439697266\n",
      "\tgrad: 2.0 4.0 -4.285204887390137\n",
      "\tgrad: 3.0 6.0 -8.870372772216797\n",
      "progress: 2 2.1856532096862793\n",
      "predict (after training): 4 6.383620738983154\n",
      "\tgrad: 1.0 2.0 -0.8081896305084229\n",
      "\tgrad: 2.0 4.0 -3.1681032180786133\n",
      "\tgrad: 3.0 6.0 -6.557973861694336\n",
      "progress: 3 1.1946394443511963\n",
      "predict (after training): 4 6.804991722106934\n",
      "\tgrad: 1.0 2.0 -0.5975041389465332\n",
      "\tgrad: 2.0 4.0 -2.3422164916992188\n",
      "\tgrad: 3.0 6.0 -4.848389625549316\n",
      "progress: 4 0.6529689431190491\n",
      "predict (after training): 4 7.116515636444092\n",
      "\tgrad: 1.0 2.0 -0.4417421817779541\n",
      "\tgrad: 2.0 4.0 -1.7316293716430664\n",
      "\tgrad: 3.0 6.0 -3.58447265625\n",
      "progress: 5 0.35690122842788696\n",
      "predict (after training): 4 7.346829414367676\n",
      "\tgrad: 1.0 2.0 -0.3265852928161621\n",
      "\tgrad: 2.0 4.0 -1.2802143096923828\n",
      "\tgrad: 3.0 6.0 -2.650045394897461\n",
      "progress: 6 0.195076122879982\n",
      "predict (after training): 4 7.51710319519043\n",
      "\tgrad: 1.0 2.0 -0.24144840240478516\n",
      "\tgrad: 2.0 4.0 -0.9464778900146484\n",
      "\tgrad: 3.0 6.0 -1.9592113494873047\n",
      "progress: 7 0.10662525147199631\n",
      "predict (after training): 4 7.642988681793213\n",
      "\tgrad: 1.0 2.0 -0.17850565910339355\n",
      "\tgrad: 2.0 4.0 -0.699742317199707\n",
      "\tgrad: 3.0 6.0 -1.4484672546386719\n",
      "progress: 8 0.0582793727517128\n",
      "predict (after training): 4 7.736057281494141\n",
      "\tgrad: 1.0 2.0 -0.1319713592529297\n",
      "\tgrad: 2.0 4.0 -0.5173273086547852\n",
      "\tgrad: 3.0 6.0 -1.070866584777832\n",
      "progress: 9 0.03185431286692619\n",
      "predict (after training): 4 7.804864406585693\n",
      "\tgrad: 1.0 2.0 -0.09756779670715332\n",
      "\tgrad: 2.0 4.0 -0.3824653625488281\n",
      "\tgrad: 3.0 6.0 -0.7917022705078125\n",
      "progress: 10 0.017410902306437492\n",
      "predict (after training): 4 7.855734348297119\n",
      "\tgrad: 1.0 2.0 -0.07213282585144043\n",
      "\tgrad: 2.0 4.0 -0.2827606201171875\n",
      "\tgrad: 3.0 6.0 -0.5853137969970703\n",
      "progress: 11 0.009516451507806778\n",
      "predict (after training): 4 7.893342971801758\n",
      "\tgrad: 1.0 2.0 -0.053328514099121094\n",
      "\tgrad: 2.0 4.0 -0.2090473175048828\n",
      "\tgrad: 3.0 6.0 -0.43272972106933594\n",
      "progress: 12 0.005201528314501047\n",
      "predict (after training): 4 7.921147346496582\n",
      "\tgrad: 1.0 2.0 -0.039426326751708984\n",
      "\tgrad: 2.0 4.0 -0.15455150604248047\n",
      "\tgrad: 3.0 6.0 -0.3199195861816406\n",
      "progress: 13 0.0028430151287466288\n",
      "predict (after training): 4 7.9417033195495605\n",
      "\tgrad: 1.0 2.0 -0.029148340225219727\n",
      "\tgrad: 2.0 4.0 -0.11426162719726562\n",
      "\tgrad: 3.0 6.0 -0.23652076721191406\n",
      "progress: 14 0.0015539465239271522\n",
      "predict (after training): 4 7.956900596618652\n",
      "\tgrad: 1.0 2.0 -0.021549701690673828\n",
      "\tgrad: 2.0 4.0 -0.08447456359863281\n",
      "\tgrad: 3.0 6.0 -0.17486286163330078\n",
      "progress: 15 0.0008493617060594261\n",
      "predict (after training): 4 7.968136310577393\n",
      "\tgrad: 1.0 2.0 -0.01593184471130371\n",
      "\tgrad: 2.0 4.0 -0.062453269958496094\n",
      "\tgrad: 3.0 6.0 -0.12927818298339844\n",
      "progress: 16 0.00046424579340964556\n",
      "predict (after training): 4 7.976442813873291\n",
      "\tgrad: 1.0 2.0 -0.011778593063354492\n",
      "\tgrad: 2.0 4.0 -0.046172142028808594\n",
      "\tgrad: 3.0 6.0 -0.09557533264160156\n",
      "progress: 17 0.0002537401160225272\n",
      "predict (after training): 4 7.982583522796631\n",
      "\tgrad: 1.0 2.0 -0.00870823860168457\n",
      "\tgrad: 2.0 4.0 -0.03413581848144531\n",
      "\tgrad: 3.0 6.0 -0.07066154479980469\n",
      "progress: 18 0.00013869594840798527\n",
      "predict (after training): 4 7.987124443054199\n",
      "\tgrad: 1.0 2.0 -0.006437778472900391\n",
      "\tgrad: 2.0 4.0 -0.025236129760742188\n",
      "\tgrad: 3.0 6.0 -0.052239418029785156\n",
      "progress: 19 7.580435340059921e-05\n",
      "predict (after training): 4 7.990480899810791\n",
      "\tgrad: 1.0 2.0 -0.004759550094604492\n",
      "\tgrad: 2.0 4.0 -0.018657684326171875\n",
      "\tgrad: 3.0 6.0 -0.038620948791503906\n",
      "progress: 20 4.143271507928148e-05\n",
      "predict (after training): 4 7.99296236038208\n",
      "\tgrad: 1.0 2.0 -0.003518819808959961\n",
      "\tgrad: 2.0 4.0 -0.0137939453125\n",
      "\tgrad: 3.0 6.0 -0.028553009033203125\n",
      "progress: 21 2.264650902361609e-05\n",
      "predict (after training): 4 7.9947967529296875\n",
      "\tgrad: 1.0 2.0 -0.00260162353515625\n",
      "\tgrad: 2.0 4.0 -0.010198593139648438\n",
      "\tgrad: 3.0 6.0 -0.021108627319335938\n",
      "progress: 22 1.2377059647405986e-05\n",
      "predict (after training): 4 7.996153354644775\n",
      "\tgrad: 1.0 2.0 -0.0019233226776123047\n",
      "\tgrad: 2.0 4.0 -0.0075397491455078125\n",
      "\tgrad: 3.0 6.0 -0.0156097412109375\n",
      "progress: 23 6.768445018678904e-06\n",
      "predict (after training): 4 7.997155666351318\n",
      "\tgrad: 1.0 2.0 -0.0014221668243408203\n",
      "\tgrad: 2.0 4.0 -0.0055751800537109375\n",
      "\tgrad: 3.0 6.0 -0.011541366577148438\n",
      "progress: 24 3.7000872907810844e-06\n",
      "predict (after training): 4 7.997897148132324\n",
      "\tgrad: 1.0 2.0 -0.0010514259338378906\n",
      "\tgrad: 2.0 4.0 -0.0041217803955078125\n",
      "\tgrad: 3.0 6.0 -0.008531570434570312\n",
      "progress: 25 2.021880391112063e-06\n",
      "predict (after training): 4 7.998445510864258\n",
      "\tgrad: 1.0 2.0 -0.0007772445678710938\n",
      "\tgrad: 2.0 4.0 -0.0030469894409179688\n",
      "\tgrad: 3.0 6.0 -0.006305694580078125\n",
      "progress: 26 1.1044940038118511e-06\n",
      "predict (after training): 4 7.9988508224487305\n",
      "\tgrad: 1.0 2.0 -0.0005745887756347656\n",
      "\tgrad: 2.0 4.0 -0.0022525787353515625\n",
      "\tgrad: 3.0 6.0 -0.0046634674072265625\n",
      "progress: 27 6.041091182851233e-07\n",
      "predict (after training): 4 7.999150276184082\n",
      "\tgrad: 1.0 2.0 -0.0004248619079589844\n",
      "\tgrad: 2.0 4.0 -0.0016651153564453125\n",
      "\tgrad: 3.0 6.0 -0.003444671630859375\n",
      "progress: 28 3.296045179013163e-07\n",
      "predict (after training): 4 7.9993720054626465\n",
      "\tgrad: 1.0 2.0 -0.0003139972686767578\n",
      "\tgrad: 2.0 4.0 -0.0012311935424804688\n",
      "\tgrad: 3.0 6.0 -0.0025491714477539062\n",
      "progress: 29 1.805076408345485e-07\n",
      "predict (after training): 4 7.99953556060791\n",
      "\tgrad: 1.0 2.0 -0.00023221969604492188\n",
      "\tgrad: 2.0 4.0 -0.0009107589721679688\n",
      "\tgrad: 3.0 6.0 -0.0018854141235351562\n",
      "progress: 30 9.874406714516226e-08\n",
      "predict (after training): 4 7.9996562004089355\n",
      "\tgrad: 1.0 2.0 -0.00017189979553222656\n",
      "\tgrad: 2.0 4.0 -0.0006742477416992188\n",
      "\tgrad: 3.0 6.0 -0.00139617919921875\n",
      "progress: 31 5.4147676564753056e-08\n",
      "predict (after training): 4 7.999745845794678\n",
      "\tgrad: 1.0 2.0 -0.0001270771026611328\n",
      "\tgrad: 2.0 4.0 -0.0004978179931640625\n",
      "\tgrad: 3.0 6.0 -0.00102996826171875\n",
      "progress: 32 2.9467628337442875e-08\n",
      "predict (after training): 4 7.999812126159668\n",
      "\tgrad: 1.0 2.0 -9.393692016601562e-05\n",
      "\tgrad: 2.0 4.0 -0.0003681182861328125\n",
      "\tgrad: 3.0 6.0 -0.0007610321044921875\n",
      "progress: 33 1.6088051779661328e-08\n",
      "predict (after training): 4 7.999861240386963\n",
      "\tgrad: 1.0 2.0 -6.937980651855469e-05\n",
      "\tgrad: 2.0 4.0 -0.00027179718017578125\n",
      "\tgrad: 3.0 6.0 -0.000560760498046875\n",
      "progress: 34 8.734787115827203e-09\n",
      "predict (after training): 4 7.999897480010986\n",
      "\tgrad: 1.0 2.0 -5.125999450683594e-05\n",
      "\tgrad: 2.0 4.0 -0.00020122528076171875\n",
      "\tgrad: 3.0 6.0 -0.0004177093505859375\n",
      "progress: 35 4.8466972657479346e-09\n",
      "predict (after training): 4 7.999924182891846\n",
      "\tgrad: 1.0 2.0 -3.790855407714844e-05\n",
      "\tgrad: 2.0 4.0 -0.000148773193359375\n",
      "\tgrad: 3.0 6.0 -0.000308990478515625\n",
      "progress: 36 2.6520865503698587e-09\n",
      "predict (after training): 4 7.999943733215332\n",
      "\tgrad: 1.0 2.0 -2.8133392333984375e-05\n",
      "\tgrad: 2.0 4.0 -0.000110626220703125\n",
      "\tgrad: 3.0 6.0 -0.0002288818359375\n",
      "progress: 37 1.4551915228366852e-09\n",
      "predict (after training): 4 7.999958038330078\n",
      "\tgrad: 1.0 2.0 -2.09808349609375e-05\n",
      "\tgrad: 2.0 4.0 -8.20159912109375e-05\n",
      "\tgrad: 3.0 6.0 -0.00016880035400390625\n",
      "progress: 38 7.914877642178908e-10\n",
      "predict (after training): 4 7.999969005584717\n",
      "\tgrad: 1.0 2.0 -1.5497207641601562e-05\n",
      "\tgrad: 2.0 4.0 -6.103515625e-05\n",
      "\tgrad: 3.0 6.0 -0.000125885009765625\n",
      "progress: 39 4.4019543565809727e-10\n",
      "predict (after training): 4 7.999977111816406\n",
      "\tgrad: 1.0 2.0 -1.1444091796875e-05\n",
      "\tgrad: 2.0 4.0 -4.482269287109375e-05\n",
      "\tgrad: 3.0 6.0 -9.1552734375e-05\n",
      "progress: 40 2.3283064365386963e-10\n",
      "predict (after training): 4 7.999983310699463\n",
      "\tgrad: 1.0 2.0 -8.344650268554688e-06\n",
      "\tgrad: 2.0 4.0 -3.24249267578125e-05\n",
      "\tgrad: 3.0 6.0 -6.580352783203125e-05\n",
      "progress: 41 1.2028067430946976e-10\n",
      "predict (after training): 4 7.999988079071045\n",
      "\tgrad: 1.0 2.0 -5.9604644775390625e-06\n",
      "\tgrad: 2.0 4.0 -2.288818359375e-05\n",
      "\tgrad: 3.0 6.0 -4.57763671875e-05\n",
      "progress: 42 5.820766091346741e-11\n",
      "predict (after training): 4 7.999991416931152\n",
      "\tgrad: 1.0 2.0 -4.291534423828125e-06\n",
      "\tgrad: 2.0 4.0 -1.71661376953125e-05\n",
      "\tgrad: 3.0 6.0 -3.719329833984375e-05\n",
      "progress: 43 3.842615114990622e-11\n",
      "predict (after training): 4 7.999993324279785\n",
      "\tgrad: 1.0 2.0 -3.337860107421875e-06\n",
      "\tgrad: 2.0 4.0 -1.33514404296875e-05\n",
      "\tgrad: 3.0 6.0 -2.86102294921875e-05\n",
      "progress: 44 2.2737367544323206e-11\n",
      "predict (after training): 4 7.99999475479126\n",
      "\tgrad: 1.0 2.0 -2.6226043701171875e-06\n",
      "\tgrad: 2.0 4.0 -1.049041748046875e-05\n",
      "\tgrad: 3.0 6.0 -2.288818359375e-05\n",
      "progress: 45 1.4551915228366852e-11\n",
      "predict (after training): 4 7.999996185302734\n",
      "\tgrad: 1.0 2.0 -1.9073486328125e-06\n",
      "\tgrad: 2.0 4.0 -7.62939453125e-06\n",
      "\tgrad: 3.0 6.0 -1.430511474609375e-05\n",
      "progress: 46 5.6843418860808015e-12\n",
      "predict (after training): 4 7.999997138977051\n",
      "\tgrad: 1.0 2.0 -1.430511474609375e-06\n",
      "\tgrad: 2.0 4.0 -5.7220458984375e-06\n",
      "\tgrad: 3.0 6.0 -1.1444091796875e-05\n",
      "progress: 47 3.637978807091713e-12\n",
      "predict (after training): 4 7.999997615814209\n",
      "\tgrad: 1.0 2.0 -1.1920928955078125e-06\n",
      "\tgrad: 2.0 4.0 -4.76837158203125e-06\n",
      "\tgrad: 3.0 6.0 -1.1444091796875e-05\n",
      "progress: 48 3.637978807091713e-12\n",
      "predict (after training): 4 7.999998092651367\n",
      "\tgrad: 1.0 2.0 -9.5367431640625e-07\n",
      "\tgrad: 2.0 4.0 -3.814697265625e-06\n",
      "\tgrad: 3.0 6.0 -8.58306884765625e-06\n",
      "progress: 49 2.0463630789890885e-12\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 50 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 51 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 52 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 53 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 54 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 55 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 56 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 57 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 58 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 59 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 60 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 61 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 62 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 63 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 64 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 65 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 66 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 67 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 68 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 69 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 70 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 71 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 72 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 73 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 74 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 75 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 76 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 77 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 78 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 79 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 80 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 81 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 82 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 83 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 84 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 85 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 86 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 87 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 88 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 89 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 90 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 91 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 92 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 93 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 94 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 95 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 96 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 97 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 98 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n",
      "\tgrad: 1.0 2.0 -7.152557373046875e-07\n",
      "\tgrad: 2.0 4.0 -2.86102294921875e-06\n",
      "\tgrad: 3.0 6.0 -5.7220458984375e-06\n",
      "progress: 99 9.094947017729282e-13\n",
      "predict (after training): 4 7.999998569488525\n"
     ]
    }
   ],
   "source": [
    "x_data=[1.0,2.0,3.0]\n",
    "y_data=[2.0,4.0,6.0]\n",
    "w=torch.Tensor([1.0])#创建张量，并初始化为1.0\n",
    "w.requires_grad=True#需要计算梯度\n",
    "def forward(x):\n",
    "    return x*w #此时的w为张量，但x可能不是张量，所以x会自动转换成张量进行数乘\n",
    "                #返回的计算值自动需要计算梯度，因为w设置为了需要计算梯度\n",
    "# torch是动态的构建计算图\n",
    "def loss(x,y):\n",
    "    y_pred=forward(x)\n",
    "    return (y_pred-y)**2\n",
    "print('predict (before training)',4,forward(4).item())\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x,y in zip(x_data,y_data):\n",
    "        l=loss(x,y)\n",
    "        l.backward()# 自动计算反向传播\n",
    "        print('\\tgrad:',x,y,w.grad.item())\n",
    "        w.data=w.data-0.01*w.grad.data# 更新w张量里的值，必须用data\n",
    "        w.grad.data.zero_()#对当前的w.grad 的值归零，下次计算会有一个新的w.grad，若不将之前的清零，\n",
    "                        #后面计算的就会跟前面计算的叠加在一起\n",
    "        # 计算完以后计算图会被释放\n",
    "    print('progress:',epoch,l.item())\n",
    "    print('predict (after training):',4,forward(4).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(47.4305, grad_fn=<MseLossBackward>)\n",
      "1 tensor(21.2197, grad_fn=<MseLossBackward>)\n",
      "2 tensor(9.5498, grad_fn=<MseLossBackward>)\n",
      "3 tensor(4.3533, grad_fn=<MseLossBackward>)\n",
      "4 tensor(2.0385, grad_fn=<MseLossBackward>)\n",
      "5 tensor(1.0065, grad_fn=<MseLossBackward>)\n",
      "6 tensor(0.5457, grad_fn=<MseLossBackward>)\n",
      "7 tensor(0.3392, grad_fn=<MseLossBackward>)\n",
      "8 tensor(0.2458, grad_fn=<MseLossBackward>)\n",
      "9 tensor(0.2029, grad_fn=<MseLossBackward>)\n",
      "10 tensor(0.1825, grad_fn=<MseLossBackward>)\n",
      "11 tensor(0.1721, grad_fn=<MseLossBackward>)\n",
      "12 tensor(0.1661, grad_fn=<MseLossBackward>)\n",
      "13 tensor(0.1622, grad_fn=<MseLossBackward>)\n",
      "14 tensor(0.1592, grad_fn=<MseLossBackward>)\n",
      "15 tensor(0.1566, grad_fn=<MseLossBackward>)\n",
      "16 tensor(0.1542, grad_fn=<MseLossBackward>)\n",
      "17 tensor(0.1519, grad_fn=<MseLossBackward>)\n",
      "18 tensor(0.1497, grad_fn=<MseLossBackward>)\n",
      "19 tensor(0.1475, grad_fn=<MseLossBackward>)\n",
      "20 tensor(0.1454, grad_fn=<MseLossBackward>)\n",
      "21 tensor(0.1433, grad_fn=<MseLossBackward>)\n",
      "22 tensor(0.1412, grad_fn=<MseLossBackward>)\n",
      "23 tensor(0.1392, grad_fn=<MseLossBackward>)\n",
      "24 tensor(0.1372, grad_fn=<MseLossBackward>)\n",
      "25 tensor(0.1352, grad_fn=<MseLossBackward>)\n",
      "26 tensor(0.1333, grad_fn=<MseLossBackward>)\n",
      "27 tensor(0.1314, grad_fn=<MseLossBackward>)\n",
      "28 tensor(0.1295, grad_fn=<MseLossBackward>)\n",
      "29 tensor(0.1276, grad_fn=<MseLossBackward>)\n",
      "30 tensor(0.1258, grad_fn=<MseLossBackward>)\n",
      "31 tensor(0.1240, grad_fn=<MseLossBackward>)\n",
      "32 tensor(0.1222, grad_fn=<MseLossBackward>)\n",
      "33 tensor(0.1205, grad_fn=<MseLossBackward>)\n",
      "34 tensor(0.1187, grad_fn=<MseLossBackward>)\n",
      "35 tensor(0.1170, grad_fn=<MseLossBackward>)\n",
      "36 tensor(0.1153, grad_fn=<MseLossBackward>)\n",
      "37 tensor(0.1137, grad_fn=<MseLossBackward>)\n",
      "38 tensor(0.1120, grad_fn=<MseLossBackward>)\n",
      "39 tensor(0.1104, grad_fn=<MseLossBackward>)\n",
      "40 tensor(0.1088, grad_fn=<MseLossBackward>)\n",
      "41 tensor(0.1073, grad_fn=<MseLossBackward>)\n",
      "42 tensor(0.1057, grad_fn=<MseLossBackward>)\n",
      "43 tensor(0.1042, grad_fn=<MseLossBackward>)\n",
      "44 tensor(0.1027, grad_fn=<MseLossBackward>)\n",
      "45 tensor(0.1012, grad_fn=<MseLossBackward>)\n",
      "46 tensor(0.0998, grad_fn=<MseLossBackward>)\n",
      "47 tensor(0.0984, grad_fn=<MseLossBackward>)\n",
      "48 tensor(0.0969, grad_fn=<MseLossBackward>)\n",
      "49 tensor(0.0955, grad_fn=<MseLossBackward>)\n",
      "50 tensor(0.0942, grad_fn=<MseLossBackward>)\n",
      "51 tensor(0.0928, grad_fn=<MseLossBackward>)\n",
      "52 tensor(0.0915, grad_fn=<MseLossBackward>)\n",
      "53 tensor(0.0902, grad_fn=<MseLossBackward>)\n",
      "54 tensor(0.0889, grad_fn=<MseLossBackward>)\n",
      "55 tensor(0.0876, grad_fn=<MseLossBackward>)\n",
      "56 tensor(0.0863, grad_fn=<MseLossBackward>)\n",
      "57 tensor(0.0851, grad_fn=<MseLossBackward>)\n",
      "58 tensor(0.0839, grad_fn=<MseLossBackward>)\n",
      "59 tensor(0.0827, grad_fn=<MseLossBackward>)\n",
      "60 tensor(0.0815, grad_fn=<MseLossBackward>)\n",
      "61 tensor(0.0803, grad_fn=<MseLossBackward>)\n",
      "62 tensor(0.0792, grad_fn=<MseLossBackward>)\n",
      "63 tensor(0.0780, grad_fn=<MseLossBackward>)\n",
      "64 tensor(0.0769, grad_fn=<MseLossBackward>)\n",
      "65 tensor(0.0758, grad_fn=<MseLossBackward>)\n",
      "66 tensor(0.0747, grad_fn=<MseLossBackward>)\n",
      "67 tensor(0.0736, grad_fn=<MseLossBackward>)\n",
      "68 tensor(0.0726, grad_fn=<MseLossBackward>)\n",
      "69 tensor(0.0715, grad_fn=<MseLossBackward>)\n",
      "70 tensor(0.0705, grad_fn=<MseLossBackward>)\n",
      "71 tensor(0.0695, grad_fn=<MseLossBackward>)\n",
      "72 tensor(0.0685, grad_fn=<MseLossBackward>)\n",
      "73 tensor(0.0675, grad_fn=<MseLossBackward>)\n",
      "74 tensor(0.0665, grad_fn=<MseLossBackward>)\n",
      "75 tensor(0.0656, grad_fn=<MseLossBackward>)\n",
      "76 tensor(0.0646, grad_fn=<MseLossBackward>)\n",
      "77 tensor(0.0637, grad_fn=<MseLossBackward>)\n",
      "78 tensor(0.0628, grad_fn=<MseLossBackward>)\n",
      "79 tensor(0.0619, grad_fn=<MseLossBackward>)\n",
      "80 tensor(0.0610, grad_fn=<MseLossBackward>)\n",
      "81 tensor(0.0601, grad_fn=<MseLossBackward>)\n",
      "82 tensor(0.0593, grad_fn=<MseLossBackward>)\n",
      "83 tensor(0.0584, grad_fn=<MseLossBackward>)\n",
      "84 tensor(0.0576, grad_fn=<MseLossBackward>)\n",
      "85 tensor(0.0567, grad_fn=<MseLossBackward>)\n",
      "86 tensor(0.0559, grad_fn=<MseLossBackward>)\n",
      "87 tensor(0.0551, grad_fn=<MseLossBackward>)\n",
      "88 tensor(0.0543, grad_fn=<MseLossBackward>)\n",
      "89 tensor(0.0535, grad_fn=<MseLossBackward>)\n",
      "90 tensor(0.0528, grad_fn=<MseLossBackward>)\n",
      "91 tensor(0.0520, grad_fn=<MseLossBackward>)\n",
      "92 tensor(0.0513, grad_fn=<MseLossBackward>)\n",
      "93 tensor(0.0505, grad_fn=<MseLossBackward>)\n",
      "94 tensor(0.0498, grad_fn=<MseLossBackward>)\n",
      "95 tensor(0.0491, grad_fn=<MseLossBackward>)\n",
      "96 tensor(0.0484, grad_fn=<MseLossBackward>)\n",
      "97 tensor(0.0477, grad_fn=<MseLossBackward>)\n",
      "98 tensor(0.0470, grad_fn=<MseLossBackward>)\n",
      "99 tensor(0.0463, grad_fn=<MseLossBackward>)\n",
      "100 tensor(0.0457, grad_fn=<MseLossBackward>)\n",
      "101 tensor(0.0450, grad_fn=<MseLossBackward>)\n",
      "102 tensor(0.0444, grad_fn=<MseLossBackward>)\n",
      "103 tensor(0.0437, grad_fn=<MseLossBackward>)\n",
      "104 tensor(0.0431, grad_fn=<MseLossBackward>)\n",
      "105 tensor(0.0425, grad_fn=<MseLossBackward>)\n",
      "106 tensor(0.0419, grad_fn=<MseLossBackward>)\n",
      "107 tensor(0.0413, grad_fn=<MseLossBackward>)\n",
      "108 tensor(0.0407, grad_fn=<MseLossBackward>)\n",
      "109 tensor(0.0401, grad_fn=<MseLossBackward>)\n",
      "110 tensor(0.0395, grad_fn=<MseLossBackward>)\n",
      "111 tensor(0.0389, grad_fn=<MseLossBackward>)\n",
      "112 tensor(0.0384, grad_fn=<MseLossBackward>)\n",
      "113 tensor(0.0378, grad_fn=<MseLossBackward>)\n",
      "114 tensor(0.0373, grad_fn=<MseLossBackward>)\n",
      "115 tensor(0.0368, grad_fn=<MseLossBackward>)\n",
      "116 tensor(0.0362, grad_fn=<MseLossBackward>)\n",
      "117 tensor(0.0357, grad_fn=<MseLossBackward>)\n",
      "118 tensor(0.0352, grad_fn=<MseLossBackward>)\n",
      "119 tensor(0.0347, grad_fn=<MseLossBackward>)\n",
      "120 tensor(0.0342, grad_fn=<MseLossBackward>)\n",
      "121 tensor(0.0337, grad_fn=<MseLossBackward>)\n",
      "122 tensor(0.0332, grad_fn=<MseLossBackward>)\n",
      "123 tensor(0.0327, grad_fn=<MseLossBackward>)\n",
      "124 tensor(0.0323, grad_fn=<MseLossBackward>)\n",
      "125 tensor(0.0318, grad_fn=<MseLossBackward>)\n",
      "126 tensor(0.0313, grad_fn=<MseLossBackward>)\n",
      "127 tensor(0.0309, grad_fn=<MseLossBackward>)\n",
      "128 tensor(0.0304, grad_fn=<MseLossBackward>)\n",
      "129 tensor(0.0300, grad_fn=<MseLossBackward>)\n",
      "130 tensor(0.0296, grad_fn=<MseLossBackward>)\n",
      "131 tensor(0.0292, grad_fn=<MseLossBackward>)\n",
      "132 tensor(0.0287, grad_fn=<MseLossBackward>)\n",
      "133 tensor(0.0283, grad_fn=<MseLossBackward>)\n",
      "134 tensor(0.0279, grad_fn=<MseLossBackward>)\n",
      "135 tensor(0.0275, grad_fn=<MseLossBackward>)\n",
      "136 tensor(0.0271, grad_fn=<MseLossBackward>)\n",
      "137 tensor(0.0267, grad_fn=<MseLossBackward>)\n",
      "138 tensor(0.0263, grad_fn=<MseLossBackward>)\n",
      "139 tensor(0.0260, grad_fn=<MseLossBackward>)\n",
      "140 tensor(0.0256, grad_fn=<MseLossBackward>)\n",
      "141 tensor(0.0252, grad_fn=<MseLossBackward>)\n",
      "142 tensor(0.0249, grad_fn=<MseLossBackward>)\n",
      "143 tensor(0.0245, grad_fn=<MseLossBackward>)\n",
      "144 tensor(0.0242, grad_fn=<MseLossBackward>)\n",
      "145 tensor(0.0238, grad_fn=<MseLossBackward>)\n",
      "146 tensor(0.0235, grad_fn=<MseLossBackward>)\n",
      "147 tensor(0.0231, grad_fn=<MseLossBackward>)\n",
      "148 tensor(0.0228, grad_fn=<MseLossBackward>)\n",
      "149 tensor(0.0225, grad_fn=<MseLossBackward>)\n",
      "150 tensor(0.0221, grad_fn=<MseLossBackward>)\n",
      "151 tensor(0.0218, grad_fn=<MseLossBackward>)\n",
      "152 tensor(0.0215, grad_fn=<MseLossBackward>)\n",
      "153 tensor(0.0212, grad_fn=<MseLossBackward>)\n",
      "154 tensor(0.0209, grad_fn=<MseLossBackward>)\n",
      "155 tensor(0.0206, grad_fn=<MseLossBackward>)\n",
      "156 tensor(0.0203, grad_fn=<MseLossBackward>)\n",
      "157 tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "158 tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "159 tensor(0.0194, grad_fn=<MseLossBackward>)\n",
      "160 tensor(0.0192, grad_fn=<MseLossBackward>)\n",
      "161 tensor(0.0189, grad_fn=<MseLossBackward>)\n",
      "162 tensor(0.0186, grad_fn=<MseLossBackward>)\n",
      "163 tensor(0.0183, grad_fn=<MseLossBackward>)\n",
      "164 tensor(0.0181, grad_fn=<MseLossBackward>)\n",
      "165 tensor(0.0178, grad_fn=<MseLossBackward>)\n",
      "166 tensor(0.0176, grad_fn=<MseLossBackward>)\n",
      "167 tensor(0.0173, grad_fn=<MseLossBackward>)\n",
      "168 tensor(0.0171, grad_fn=<MseLossBackward>)\n",
      "169 tensor(0.0168, grad_fn=<MseLossBackward>)\n",
      "170 tensor(0.0166, grad_fn=<MseLossBackward>)\n",
      "171 tensor(0.0163, grad_fn=<MseLossBackward>)\n",
      "172 tensor(0.0161, grad_fn=<MseLossBackward>)\n",
      "173 tensor(0.0159, grad_fn=<MseLossBackward>)\n",
      "174 tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "175 tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "176 tensor(0.0152, grad_fn=<MseLossBackward>)\n",
      "177 tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "178 tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "179 tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "180 tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "181 tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "182 tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "183 tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "184 tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "185 tensor(0.0133, grad_fn=<MseLossBackward>)\n",
      "186 tensor(0.0132, grad_fn=<MseLossBackward>)\n",
      "187 tensor(0.0130, grad_fn=<MseLossBackward>)\n",
      "188 tensor(0.0128, grad_fn=<MseLossBackward>)\n",
      "189 tensor(0.0126, grad_fn=<MseLossBackward>)\n",
      "190 tensor(0.0124, grad_fn=<MseLossBackward>)\n",
      "191 tensor(0.0122, grad_fn=<MseLossBackward>)\n",
      "192 tensor(0.0121, grad_fn=<MseLossBackward>)\n",
      "193 tensor(0.0119, grad_fn=<MseLossBackward>)\n",
      "194 tensor(0.0117, grad_fn=<MseLossBackward>)\n",
      "195 tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "196 tensor(0.0114, grad_fn=<MseLossBackward>)\n",
      "197 tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "198 tensor(0.0111, grad_fn=<MseLossBackward>)\n",
      "199 tensor(0.0109, grad_fn=<MseLossBackward>)\n",
      "200 tensor(0.0107, grad_fn=<MseLossBackward>)\n",
      "201 tensor(0.0106, grad_fn=<MseLossBackward>)\n",
      "202 tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "203 tensor(0.0103, grad_fn=<MseLossBackward>)\n",
      "204 tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "205 tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "206 tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "207 tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "208 tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "209 tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "210 tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "211 tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "212 tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "213 tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "214 tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "215 tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "216 tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "217 tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "218 tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "219 tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "220 tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "221 tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "222 tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "223 tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "224 tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "225 tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "226 tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "227 tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "228 tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "229 tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "230 tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "231 tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "232 tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "233 tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "234 tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "235 tensor(0.0065, grad_fn=<MseLossBackward>)\n",
      "236 tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "237 tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "238 tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "239 tensor(0.0061, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "241 tensor(0.0059, grad_fn=<MseLossBackward>)\n",
      "242 tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "243 tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "244 tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "245 tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "246 tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "247 tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "248 tensor(0.0054, grad_fn=<MseLossBackward>)\n",
      "249 tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "250 tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "251 tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "252 tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "253 tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "254 tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "255 tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "256 tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "257 tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "258 tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "259 tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "260 tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "261 tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "262 tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "263 tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "264 tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "265 tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "266 tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "267 tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "268 tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "269 tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "270 tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "271 tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "272 tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "273 tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "274 tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "275 tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "276 tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "277 tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "278 tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "279 tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "280 tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "281 tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "282 tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "283 tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "284 tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "285 tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "286 tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "287 tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "288 tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "289 tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "290 tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "291 tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "292 tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "293 tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "294 tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "295 tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "296 tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "297 tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "298 tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "299 tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "300 tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "301 tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "302 tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "303 tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "304 tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "305 tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "306 tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "307 tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "308 tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "309 tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "310 tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "311 tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "312 tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "313 tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "314 tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "315 tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "316 tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "317 tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "318 tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "319 tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "320 tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "321 tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "322 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "323 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "324 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "325 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "326 tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "327 tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "328 tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "329 tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "330 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "331 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "332 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "333 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "334 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "335 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "336 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "337 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "338 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "339 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "340 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "341 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "342 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "343 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "344 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "345 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "346 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "347 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "348 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "349 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "350 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "351 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "352 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "353 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "354 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "355 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "356 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "357 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "358 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "359 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "360 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "361 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "362 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "363 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "364 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "365 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "366 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "367 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "368 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "369 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "370 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "371 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "372 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "373 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "374 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "375 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "376 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "377 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "378 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "379 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "380 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "381 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "382 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "383 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "384 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "385 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "386 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "387 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "388 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "389 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "390 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "391 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "392 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "393 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "394 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "395 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "396 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "397 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "398 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "399 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "400 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "401 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "402 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "403 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "404 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "405 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "406 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "407 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "408 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "409 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "410 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "411 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "412 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "413 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "414 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "415 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "416 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "417 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "418 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "419 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "420 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "421 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "422 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "423 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "424 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "425 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "426 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "427 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "428 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "429 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "430 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "431 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "432 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "433 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "434 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "435 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "436 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "437 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "438 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "439 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "440 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "441 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "442 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "443 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "444 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "445 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "446 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "447 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "448 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "449 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "450 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "451 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "452 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "453 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "454 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "455 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "456 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "457 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "458 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "459 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "460 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "461 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "462 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "463 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "464 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "465 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "466 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "467 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "468 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "469 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "470 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "471 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "472 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "473 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "474 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "475 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "476 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "477 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "478 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "479 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "480 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "481 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "482 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "483 tensor(0.0002, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "485 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "486 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "487 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "488 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "489 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "490 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "491 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "492 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "493 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "494 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "495 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "496 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "497 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "498 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "499 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "500 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "501 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "502 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "503 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "504 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "505 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "506 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "507 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "508 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "509 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "510 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "511 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "512 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "513 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "514 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "515 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "516 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "517 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "518 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "519 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "520 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "521 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "522 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "523 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "524 tensor(9.8627e-05, grad_fn=<MseLossBackward>)\n",
      "525 tensor(9.7212e-05, grad_fn=<MseLossBackward>)\n",
      "526 tensor(9.5817e-05, grad_fn=<MseLossBackward>)\n",
      "527 tensor(9.4439e-05, grad_fn=<MseLossBackward>)\n",
      "528 tensor(9.3076e-05, grad_fn=<MseLossBackward>)\n",
      "529 tensor(9.1743e-05, grad_fn=<MseLossBackward>)\n",
      "530 tensor(9.0425e-05, grad_fn=<MseLossBackward>)\n",
      "531 tensor(8.9127e-05, grad_fn=<MseLossBackward>)\n",
      "532 tensor(8.7844e-05, grad_fn=<MseLossBackward>)\n",
      "533 tensor(8.6582e-05, grad_fn=<MseLossBackward>)\n",
      "534 tensor(8.5339e-05, grad_fn=<MseLossBackward>)\n",
      "535 tensor(8.4110e-05, grad_fn=<MseLossBackward>)\n",
      "536 tensor(8.2900e-05, grad_fn=<MseLossBackward>)\n",
      "537 tensor(8.1708e-05, grad_fn=<MseLossBackward>)\n",
      "538 tensor(8.0538e-05, grad_fn=<MseLossBackward>)\n",
      "539 tensor(7.9380e-05, grad_fn=<MseLossBackward>)\n",
      "540 tensor(7.8240e-05, grad_fn=<MseLossBackward>)\n",
      "541 tensor(7.7114e-05, grad_fn=<MseLossBackward>)\n",
      "542 tensor(7.6003e-05, grad_fn=<MseLossBackward>)\n",
      "543 tensor(7.4909e-05, grad_fn=<MseLossBackward>)\n",
      "544 tensor(7.3837e-05, grad_fn=<MseLossBackward>)\n",
      "545 tensor(7.2777e-05, grad_fn=<MseLossBackward>)\n",
      "546 tensor(7.1731e-05, grad_fn=<MseLossBackward>)\n",
      "547 tensor(7.0698e-05, grad_fn=<MseLossBackward>)\n",
      "548 tensor(6.9683e-05, grad_fn=<MseLossBackward>)\n",
      "549 tensor(6.8680e-05, grad_fn=<MseLossBackward>)\n",
      "550 tensor(6.7694e-05, grad_fn=<MseLossBackward>)\n",
      "551 tensor(6.6718e-05, grad_fn=<MseLossBackward>)\n",
      "552 tensor(6.5760e-05, grad_fn=<MseLossBackward>)\n",
      "553 tensor(6.4815e-05, grad_fn=<MseLossBackward>)\n",
      "554 tensor(6.3885e-05, grad_fn=<MseLossBackward>)\n",
      "555 tensor(6.2968e-05, grad_fn=<MseLossBackward>)\n",
      "556 tensor(6.2065e-05, grad_fn=<MseLossBackward>)\n",
      "557 tensor(6.1174e-05, grad_fn=<MseLossBackward>)\n",
      "558 tensor(6.0294e-05, grad_fn=<MseLossBackward>)\n",
      "559 tensor(5.9427e-05, grad_fn=<MseLossBackward>)\n",
      "560 tensor(5.8573e-05, grad_fn=<MseLossBackward>)\n",
      "561 tensor(5.7727e-05, grad_fn=<MseLossBackward>)\n",
      "562 tensor(5.6898e-05, grad_fn=<MseLossBackward>)\n",
      "563 tensor(5.6081e-05, grad_fn=<MseLossBackward>)\n",
      "564 tensor(5.5278e-05, grad_fn=<MseLossBackward>)\n",
      "565 tensor(5.4479e-05, grad_fn=<MseLossBackward>)\n",
      "566 tensor(5.3697e-05, grad_fn=<MseLossBackward>)\n",
      "567 tensor(5.2925e-05, grad_fn=<MseLossBackward>)\n",
      "568 tensor(5.2167e-05, grad_fn=<MseLossBackward>)\n",
      "569 tensor(5.1416e-05, grad_fn=<MseLossBackward>)\n",
      "570 tensor(5.0677e-05, grad_fn=<MseLossBackward>)\n",
      "571 tensor(4.9951e-05, grad_fn=<MseLossBackward>)\n",
      "572 tensor(4.9231e-05, grad_fn=<MseLossBackward>)\n",
      "573 tensor(4.8524e-05, grad_fn=<MseLossBackward>)\n",
      "574 tensor(4.7826e-05, grad_fn=<MseLossBackward>)\n",
      "575 tensor(4.7141e-05, grad_fn=<MseLossBackward>)\n",
      "576 tensor(4.6460e-05, grad_fn=<MseLossBackward>)\n",
      "577 tensor(4.5793e-05, grad_fn=<MseLossBackward>)\n",
      "578 tensor(4.5135e-05, grad_fn=<MseLossBackward>)\n",
      "579 tensor(4.4486e-05, grad_fn=<MseLossBackward>)\n",
      "580 tensor(4.3847e-05, grad_fn=<MseLossBackward>)\n",
      "581 tensor(4.3217e-05, grad_fn=<MseLossBackward>)\n",
      "582 tensor(4.2597e-05, grad_fn=<MseLossBackward>)\n",
      "583 tensor(4.1984e-05, grad_fn=<MseLossBackward>)\n",
      "584 tensor(4.1380e-05, grad_fn=<MseLossBackward>)\n",
      "585 tensor(4.0784e-05, grad_fn=<MseLossBackward>)\n",
      "586 tensor(4.0201e-05, grad_fn=<MseLossBackward>)\n",
      "587 tensor(3.9622e-05, grad_fn=<MseLossBackward>)\n",
      "588 tensor(3.9052e-05, grad_fn=<MseLossBackward>)\n",
      "589 tensor(3.8492e-05, grad_fn=<MseLossBackward>)\n",
      "590 tensor(3.7936e-05, grad_fn=<MseLossBackward>)\n",
      "591 tensor(3.7392e-05, grad_fn=<MseLossBackward>)\n",
      "592 tensor(3.6855e-05, grad_fn=<MseLossBackward>)\n",
      "593 tensor(3.6324e-05, grad_fn=<MseLossBackward>)\n",
      "594 tensor(3.5805e-05, grad_fn=<MseLossBackward>)\n",
      "595 tensor(3.5289e-05, grad_fn=<MseLossBackward>)\n",
      "596 tensor(3.4780e-05, grad_fn=<MseLossBackward>)\n",
      "597 tensor(3.4282e-05, grad_fn=<MseLossBackward>)\n",
      "598 tensor(3.3790e-05, grad_fn=<MseLossBackward>)\n",
      "599 tensor(3.3304e-05, grad_fn=<MseLossBackward>)\n",
      "600 tensor(3.2825e-05, grad_fn=<MseLossBackward>)\n",
      "601 tensor(3.2354e-05, grad_fn=<MseLossBackward>)\n",
      "602 tensor(3.1888e-05, grad_fn=<MseLossBackward>)\n",
      "603 tensor(3.1428e-05, grad_fn=<MseLossBackward>)\n",
      "604 tensor(3.0977e-05, grad_fn=<MseLossBackward>)\n",
      "605 tensor(3.0534e-05, grad_fn=<MseLossBackward>)\n",
      "606 tensor(3.0096e-05, grad_fn=<MseLossBackward>)\n",
      "607 tensor(2.9662e-05, grad_fn=<MseLossBackward>)\n",
      "608 tensor(2.9235e-05, grad_fn=<MseLossBackward>)\n",
      "609 tensor(2.8816e-05, grad_fn=<MseLossBackward>)\n",
      "610 tensor(2.8402e-05, grad_fn=<MseLossBackward>)\n",
      "611 tensor(2.7994e-05, grad_fn=<MseLossBackward>)\n",
      "612 tensor(2.7591e-05, grad_fn=<MseLossBackward>)\n",
      "613 tensor(2.7195e-05, grad_fn=<MseLossBackward>)\n",
      "614 tensor(2.6805e-05, grad_fn=<MseLossBackward>)\n",
      "615 tensor(2.6416e-05, grad_fn=<MseLossBackward>)\n",
      "616 tensor(2.6038e-05, grad_fn=<MseLossBackward>)\n",
      "617 tensor(2.5663e-05, grad_fn=<MseLossBackward>)\n",
      "618 tensor(2.5294e-05, grad_fn=<MseLossBackward>)\n",
      "619 tensor(2.4930e-05, grad_fn=<MseLossBackward>)\n",
      "620 tensor(2.4574e-05, grad_fn=<MseLossBackward>)\n",
      "621 tensor(2.4220e-05, grad_fn=<MseLossBackward>)\n",
      "622 tensor(2.3873e-05, grad_fn=<MseLossBackward>)\n",
      "623 tensor(2.3530e-05, grad_fn=<MseLossBackward>)\n",
      "624 tensor(2.3190e-05, grad_fn=<MseLossBackward>)\n",
      "625 tensor(2.2858e-05, grad_fn=<MseLossBackward>)\n",
      "626 tensor(2.2529e-05, grad_fn=<MseLossBackward>)\n",
      "627 tensor(2.2206e-05, grad_fn=<MseLossBackward>)\n",
      "628 tensor(2.1886e-05, grad_fn=<MseLossBackward>)\n",
      "629 tensor(2.1572e-05, grad_fn=<MseLossBackward>)\n",
      "630 tensor(2.1263e-05, grad_fn=<MseLossBackward>)\n",
      "631 tensor(2.0955e-05, grad_fn=<MseLossBackward>)\n",
      "632 tensor(2.0655e-05, grad_fn=<MseLossBackward>)\n",
      "633 tensor(2.0357e-05, grad_fn=<MseLossBackward>)\n",
      "634 tensor(2.0065e-05, grad_fn=<MseLossBackward>)\n",
      "635 tensor(1.9777e-05, grad_fn=<MseLossBackward>)\n",
      "636 tensor(1.9491e-05, grad_fn=<MseLossBackward>)\n",
      "637 tensor(1.9212e-05, grad_fn=<MseLossBackward>)\n",
      "638 tensor(1.8935e-05, grad_fn=<MseLossBackward>)\n",
      "639 tensor(1.8662e-05, grad_fn=<MseLossBackward>)\n",
      "640 tensor(1.8397e-05, grad_fn=<MseLossBackward>)\n",
      "641 tensor(1.8133e-05, grad_fn=<MseLossBackward>)\n",
      "642 tensor(1.7871e-05, grad_fn=<MseLossBackward>)\n",
      "643 tensor(1.7614e-05, grad_fn=<MseLossBackward>)\n",
      "644 tensor(1.7361e-05, grad_fn=<MseLossBackward>)\n",
      "645 tensor(1.7111e-05, grad_fn=<MseLossBackward>)\n",
      "646 tensor(1.6867e-05, grad_fn=<MseLossBackward>)\n",
      "647 tensor(1.6623e-05, grad_fn=<MseLossBackward>)\n",
      "648 tensor(1.6384e-05, grad_fn=<MseLossBackward>)\n",
      "649 tensor(1.6150e-05, grad_fn=<MseLossBackward>)\n",
      "650 tensor(1.5918e-05, grad_fn=<MseLossBackward>)\n",
      "651 tensor(1.5689e-05, grad_fn=<MseLossBackward>)\n",
      "652 tensor(1.5462e-05, grad_fn=<MseLossBackward>)\n",
      "653 tensor(1.5241e-05, grad_fn=<MseLossBackward>)\n",
      "654 tensor(1.5023e-05, grad_fn=<MseLossBackward>)\n",
      "655 tensor(1.4806e-05, grad_fn=<MseLossBackward>)\n",
      "656 tensor(1.4593e-05, grad_fn=<MseLossBackward>)\n",
      "657 tensor(1.4383e-05, grad_fn=<MseLossBackward>)\n",
      "658 tensor(1.4178e-05, grad_fn=<MseLossBackward>)\n",
      "659 tensor(1.3972e-05, grad_fn=<MseLossBackward>)\n",
      "660 tensor(1.3772e-05, grad_fn=<MseLossBackward>)\n",
      "661 tensor(1.3574e-05, grad_fn=<MseLossBackward>)\n",
      "662 tensor(1.3378e-05, grad_fn=<MseLossBackward>)\n",
      "663 tensor(1.3186e-05, grad_fn=<MseLossBackward>)\n",
      "664 tensor(1.2998e-05, grad_fn=<MseLossBackward>)\n",
      "665 tensor(1.2810e-05, grad_fn=<MseLossBackward>)\n",
      "666 tensor(1.2628e-05, grad_fn=<MseLossBackward>)\n",
      "667 tensor(1.2446e-05, grad_fn=<MseLossBackward>)\n",
      "668 tensor(1.2268e-05, grad_fn=<MseLossBackward>)\n",
      "669 tensor(1.2090e-05, grad_fn=<MseLossBackward>)\n",
      "670 tensor(1.1917e-05, grad_fn=<MseLossBackward>)\n",
      "671 tensor(1.1744e-05, grad_fn=<MseLossBackward>)\n",
      "672 tensor(1.1576e-05, grad_fn=<MseLossBackward>)\n",
      "673 tensor(1.1411e-05, grad_fn=<MseLossBackward>)\n",
      "674 tensor(1.1247e-05, grad_fn=<MseLossBackward>)\n",
      "675 tensor(1.1084e-05, grad_fn=<MseLossBackward>)\n",
      "676 tensor(1.0926e-05, grad_fn=<MseLossBackward>)\n",
      "677 tensor(1.0769e-05, grad_fn=<MseLossBackward>)\n",
      "678 tensor(1.0613e-05, grad_fn=<MseLossBackward>)\n",
      "679 tensor(1.0462e-05, grad_fn=<MseLossBackward>)\n",
      "680 tensor(1.0310e-05, grad_fn=<MseLossBackward>)\n",
      "681 tensor(1.0163e-05, grad_fn=<MseLossBackward>)\n",
      "682 tensor(1.0017e-05, grad_fn=<MseLossBackward>)\n",
      "683 tensor(9.8739e-06, grad_fn=<MseLossBackward>)\n",
      "684 tensor(9.7313e-06, grad_fn=<MseLossBackward>)\n",
      "685 tensor(9.5907e-06, grad_fn=<MseLossBackward>)\n",
      "686 tensor(9.4515e-06, grad_fn=<MseLossBackward>)\n",
      "687 tensor(9.3170e-06, grad_fn=<MseLossBackward>)\n",
      "688 tensor(9.1831e-06, grad_fn=<MseLossBackward>)\n",
      "689 tensor(9.0509e-06, grad_fn=<MseLossBackward>)\n",
      "690 tensor(8.9221e-06, grad_fn=<MseLossBackward>)\n",
      "691 tensor(8.7925e-06, grad_fn=<MseLossBackward>)\n",
      "692 tensor(8.6667e-06, grad_fn=<MseLossBackward>)\n",
      "693 tensor(8.5419e-06, grad_fn=<MseLossBackward>)\n",
      "694 tensor(8.4191e-06, grad_fn=<MseLossBackward>)\n",
      "695 tensor(8.2972e-06, grad_fn=<MseLossBackward>)\n",
      "696 tensor(8.1788e-06, grad_fn=<MseLossBackward>)\n",
      "697 tensor(8.0624e-06, grad_fn=<MseLossBackward>)\n",
      "698 tensor(7.9443e-06, grad_fn=<MseLossBackward>)\n",
      "699 tensor(7.8308e-06, grad_fn=<MseLossBackward>)\n",
      "700 tensor(7.7185e-06, grad_fn=<MseLossBackward>)\n",
      "701 tensor(7.6089e-06, grad_fn=<MseLossBackward>)\n",
      "702 tensor(7.4981e-06, grad_fn=<MseLossBackward>)\n",
      "703 tensor(7.3899e-06, grad_fn=<MseLossBackward>)\n",
      "704 tensor(7.2839e-06, grad_fn=<MseLossBackward>)\n",
      "705 tensor(7.1790e-06, grad_fn=<MseLossBackward>)\n",
      "706 tensor(7.0755e-06, grad_fn=<MseLossBackward>)\n",
      "707 tensor(6.9744e-06, grad_fn=<MseLossBackward>)\n",
      "708 tensor(6.8738e-06, grad_fn=<MseLossBackward>)\n",
      "709 tensor(6.7760e-06, grad_fn=<MseLossBackward>)\n",
      "710 tensor(6.6780e-06, grad_fn=<MseLossBackward>)\n",
      "711 tensor(6.5817e-06, grad_fn=<MseLossBackward>)\n",
      "712 tensor(6.4875e-06, grad_fn=<MseLossBackward>)\n",
      "713 tensor(6.3938e-06, grad_fn=<MseLossBackward>)\n",
      "714 tensor(6.3037e-06, grad_fn=<MseLossBackward>)\n",
      "715 tensor(6.2123e-06, grad_fn=<MseLossBackward>)\n",
      "716 tensor(6.1231e-06, grad_fn=<MseLossBackward>)\n",
      "717 tensor(6.0338e-06, grad_fn=<MseLossBackward>)\n",
      "718 tensor(5.9479e-06, grad_fn=<MseLossBackward>)\n",
      "719 tensor(5.8623e-06, grad_fn=<MseLossBackward>)\n",
      "720 tensor(5.7794e-06, grad_fn=<MseLossBackward>)\n",
      "721 tensor(5.6953e-06, grad_fn=<MseLossBackward>)\n",
      "722 tensor(5.6125e-06, grad_fn=<MseLossBackward>)\n",
      "723 tensor(5.5327e-06, grad_fn=<MseLossBackward>)\n",
      "724 tensor(5.4531e-06, grad_fn=<MseLossBackward>)\n",
      "725 tensor(5.3755e-06, grad_fn=<MseLossBackward>)\n",
      "726 tensor(5.2969e-06, grad_fn=<MseLossBackward>)\n",
      "727 tensor(5.2213e-06, grad_fn=<MseLossBackward>)\n",
      "728 tensor(5.1473e-06, grad_fn=<MseLossBackward>)\n",
      "729 tensor(5.0724e-06, grad_fn=<MseLossBackward>)\n",
      "730 tensor(5.0003e-06, grad_fn=<MseLossBackward>)\n",
      "731 tensor(4.9278e-06, grad_fn=<MseLossBackward>)\n",
      "732 tensor(4.8577e-06, grad_fn=<MseLossBackward>)\n",
      "733 tensor(4.7871e-06, grad_fn=<MseLossBackward>)\n",
      "734 tensor(4.7189e-06, grad_fn=<MseLossBackward>)\n",
      "735 tensor(4.6512e-06, grad_fn=<MseLossBackward>)\n",
      "736 tensor(4.5839e-06, grad_fn=<MseLossBackward>)\n",
      "737 tensor(4.5181e-06, grad_fn=<MseLossBackward>)\n",
      "738 tensor(4.4527e-06, grad_fn=<MseLossBackward>)\n",
      "739 tensor(4.3897e-06, grad_fn=<MseLossBackward>)\n",
      "740 tensor(4.3261e-06, grad_fn=<MseLossBackward>)\n",
      "741 tensor(4.2639e-06, grad_fn=<MseLossBackward>)\n",
      "742 tensor(4.2030e-06, grad_fn=<MseLossBackward>)\n",
      "743 tensor(4.1429e-06, grad_fn=<MseLossBackward>)\n",
      "744 tensor(4.0820e-06, grad_fn=<MseLossBackward>)\n",
      "745 tensor(4.0236e-06, grad_fn=<MseLossBackward>)\n",
      "746 tensor(3.9661e-06, grad_fn=<MseLossBackward>)\n",
      "747 tensor(3.9100e-06, grad_fn=<MseLossBackward>)\n",
      "748 tensor(3.8527e-06, grad_fn=<MseLossBackward>)\n",
      "749 tensor(3.7975e-06, grad_fn=<MseLossBackward>)\n",
      "750 tensor(3.7429e-06, grad_fn=<MseLossBackward>)\n",
      "751 tensor(3.6891e-06, grad_fn=<MseLossBackward>)\n",
      "752 tensor(3.6359e-06, grad_fn=<MseLossBackward>)\n",
      "753 tensor(3.5837e-06, grad_fn=<MseLossBackward>)\n",
      "754 tensor(3.5322e-06, grad_fn=<MseLossBackward>)\n",
      "755 tensor(3.4818e-06, grad_fn=<MseLossBackward>)\n",
      "756 tensor(3.4317e-06, grad_fn=<MseLossBackward>)\n",
      "757 tensor(3.3829e-06, grad_fn=<MseLossBackward>)\n",
      "758 tensor(3.3327e-06, grad_fn=<MseLossBackward>)\n",
      "759 tensor(3.2853e-06, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760 tensor(3.2380e-06, grad_fn=<MseLossBackward>)\n",
      "761 tensor(3.1928e-06, grad_fn=<MseLossBackward>)\n",
      "762 tensor(3.1466e-06, grad_fn=<MseLossBackward>)\n",
      "763 tensor(3.1013e-06, grad_fn=<MseLossBackward>)\n",
      "764 tensor(3.0563e-06, grad_fn=<MseLossBackward>)\n",
      "765 tensor(3.0124e-06, grad_fn=<MseLossBackward>)\n",
      "766 tensor(2.9695e-06, grad_fn=<MseLossBackward>)\n",
      "767 tensor(2.9261e-06, grad_fn=<MseLossBackward>)\n",
      "768 tensor(2.8847e-06, grad_fn=<MseLossBackward>)\n",
      "769 tensor(2.8436e-06, grad_fn=<MseLossBackward>)\n",
      "770 tensor(2.8019e-06, grad_fn=<MseLossBackward>)\n",
      "771 tensor(2.7615e-06, grad_fn=<MseLossBackward>)\n",
      "772 tensor(2.7226e-06, grad_fn=<MseLossBackward>)\n",
      "773 tensor(2.6831e-06, grad_fn=<MseLossBackward>)\n",
      "774 tensor(2.6443e-06, grad_fn=<MseLossBackward>)\n",
      "775 tensor(2.6069e-06, grad_fn=<MseLossBackward>)\n",
      "776 tensor(2.5692e-06, grad_fn=<MseLossBackward>)\n",
      "777 tensor(2.5316e-06, grad_fn=<MseLossBackward>)\n",
      "778 tensor(2.4952e-06, grad_fn=<MseLossBackward>)\n",
      "779 tensor(2.4597e-06, grad_fn=<MseLossBackward>)\n",
      "780 tensor(2.4242e-06, grad_fn=<MseLossBackward>)\n",
      "781 tensor(2.3893e-06, grad_fn=<MseLossBackward>)\n",
      "782 tensor(2.3559e-06, grad_fn=<MseLossBackward>)\n",
      "783 tensor(2.3211e-06, grad_fn=<MseLossBackward>)\n",
      "784 tensor(2.2882e-06, grad_fn=<MseLossBackward>)\n",
      "785 tensor(2.2549e-06, grad_fn=<MseLossBackward>)\n",
      "786 tensor(2.2231e-06, grad_fn=<MseLossBackward>)\n",
      "787 tensor(2.1902e-06, grad_fn=<MseLossBackward>)\n",
      "788 tensor(2.1595e-06, grad_fn=<MseLossBackward>)\n",
      "789 tensor(2.1276e-06, grad_fn=<MseLossBackward>)\n",
      "790 tensor(2.0980e-06, grad_fn=<MseLossBackward>)\n",
      "791 tensor(2.0672e-06, grad_fn=<MseLossBackward>)\n",
      "792 tensor(2.0378e-06, grad_fn=<MseLossBackward>)\n",
      "793 tensor(2.0083e-06, grad_fn=<MseLossBackward>)\n",
      "794 tensor(1.9792e-06, grad_fn=<MseLossBackward>)\n",
      "795 tensor(1.9519e-06, grad_fn=<MseLossBackward>)\n",
      "796 tensor(1.9236e-06, grad_fn=<MseLossBackward>)\n",
      "797 tensor(1.8960e-06, grad_fn=<MseLossBackward>)\n",
      "798 tensor(1.8684e-06, grad_fn=<MseLossBackward>)\n",
      "799 tensor(1.8418e-06, grad_fn=<MseLossBackward>)\n",
      "800 tensor(1.8147e-06, grad_fn=<MseLossBackward>)\n",
      "801 tensor(1.7892e-06, grad_fn=<MseLossBackward>)\n",
      "802 tensor(1.7631e-06, grad_fn=<MseLossBackward>)\n",
      "803 tensor(1.7383e-06, grad_fn=<MseLossBackward>)\n",
      "804 tensor(1.7133e-06, grad_fn=<MseLossBackward>)\n",
      "805 tensor(1.6883e-06, grad_fn=<MseLossBackward>)\n",
      "806 tensor(1.6642e-06, grad_fn=<MseLossBackward>)\n",
      "807 tensor(1.6401e-06, grad_fn=<MseLossBackward>)\n",
      "808 tensor(1.6164e-06, grad_fn=<MseLossBackward>)\n",
      "809 tensor(1.5938e-06, grad_fn=<MseLossBackward>)\n",
      "810 tensor(1.5707e-06, grad_fn=<MseLossBackward>)\n",
      "811 tensor(1.5480e-06, grad_fn=<MseLossBackward>)\n",
      "812 tensor(1.5255e-06, grad_fn=<MseLossBackward>)\n",
      "813 tensor(1.5040e-06, grad_fn=<MseLossBackward>)\n",
      "814 tensor(1.4822e-06, grad_fn=<MseLossBackward>)\n",
      "815 tensor(1.4605e-06, grad_fn=<MseLossBackward>)\n",
      "816 tensor(1.4397e-06, grad_fn=<MseLossBackward>)\n",
      "817 tensor(1.4195e-06, grad_fn=<MseLossBackward>)\n",
      "818 tensor(1.3987e-06, grad_fn=<MseLossBackward>)\n",
      "819 tensor(1.3788e-06, grad_fn=<MseLossBackward>)\n",
      "820 tensor(1.3590e-06, grad_fn=<MseLossBackward>)\n",
      "821 tensor(1.3389e-06, grad_fn=<MseLossBackward>)\n",
      "822 tensor(1.3197e-06, grad_fn=<MseLossBackward>)\n",
      "823 tensor(1.3013e-06, grad_fn=<MseLossBackward>)\n",
      "824 tensor(1.2826e-06, grad_fn=<MseLossBackward>)\n",
      "825 tensor(1.2640e-06, grad_fn=<MseLossBackward>)\n",
      "826 tensor(1.2455e-06, grad_fn=<MseLossBackward>)\n",
      "827 tensor(1.2281e-06, grad_fn=<MseLossBackward>)\n",
      "828 tensor(1.2104e-06, grad_fn=<MseLossBackward>)\n",
      "829 tensor(1.1931e-06, grad_fn=<MseLossBackward>)\n",
      "830 tensor(1.1757e-06, grad_fn=<MseLossBackward>)\n",
      "831 tensor(1.1588e-06, grad_fn=<MseLossBackward>)\n",
      "832 tensor(1.1420e-06, grad_fn=<MseLossBackward>)\n",
      "833 tensor(1.1258e-06, grad_fn=<MseLossBackward>)\n",
      "834 tensor(1.1098e-06, grad_fn=<MseLossBackward>)\n",
      "835 tensor(1.0938e-06, grad_fn=<MseLossBackward>)\n",
      "836 tensor(1.0779e-06, grad_fn=<MseLossBackward>)\n",
      "837 tensor(1.0622e-06, grad_fn=<MseLossBackward>)\n",
      "838 tensor(1.0470e-06, grad_fn=<MseLossBackward>)\n",
      "839 tensor(1.0321e-06, grad_fn=<MseLossBackward>)\n",
      "840 tensor(1.0175e-06, grad_fn=<MseLossBackward>)\n",
      "841 tensor(1.0026e-06, grad_fn=<MseLossBackward>)\n",
      "842 tensor(9.8830e-07, grad_fn=<MseLossBackward>)\n",
      "843 tensor(9.7407e-07, grad_fn=<MseLossBackward>)\n",
      "844 tensor(9.5995e-07, grad_fn=<MseLossBackward>)\n",
      "845 tensor(9.4604e-07, grad_fn=<MseLossBackward>)\n",
      "846 tensor(9.3297e-07, grad_fn=<MseLossBackward>)\n",
      "847 tensor(9.1953e-07, grad_fn=<MseLossBackward>)\n",
      "848 tensor(9.0576e-07, grad_fn=<MseLossBackward>)\n",
      "849 tensor(8.9308e-07, grad_fn=<MseLossBackward>)\n",
      "850 tensor(8.7994e-07, grad_fn=<MseLossBackward>)\n",
      "851 tensor(8.6744e-07, grad_fn=<MseLossBackward>)\n",
      "852 tensor(8.5529e-07, grad_fn=<MseLossBackward>)\n",
      "853 tensor(8.4292e-07, grad_fn=<MseLossBackward>)\n",
      "854 tensor(8.3052e-07, grad_fn=<MseLossBackward>)\n",
      "855 tensor(8.1880e-07, grad_fn=<MseLossBackward>)\n",
      "856 tensor(8.0695e-07, grad_fn=<MseLossBackward>)\n",
      "857 tensor(7.9576e-07, grad_fn=<MseLossBackward>)\n",
      "858 tensor(7.8408e-07, grad_fn=<MseLossBackward>)\n",
      "859 tensor(7.7305e-07, grad_fn=<MseLossBackward>)\n",
      "860 tensor(7.6154e-07, grad_fn=<MseLossBackward>)\n",
      "861 tensor(7.5067e-07, grad_fn=<MseLossBackward>)\n",
      "862 tensor(7.3968e-07, grad_fn=<MseLossBackward>)\n",
      "863 tensor(7.2931e-07, grad_fn=<MseLossBackward>)\n",
      "864 tensor(7.1902e-07, grad_fn=<MseLossBackward>)\n",
      "865 tensor(7.0826e-07, grad_fn=<MseLossBackward>)\n",
      "866 tensor(6.9851e-07, grad_fn=<MseLossBackward>)\n",
      "867 tensor(6.8839e-07, grad_fn=<MseLossBackward>)\n",
      "868 tensor(6.7825e-07, grad_fn=<MseLossBackward>)\n",
      "869 tensor(6.6828e-07, grad_fn=<MseLossBackward>)\n",
      "870 tensor(6.5876e-07, grad_fn=<MseLossBackward>)\n",
      "871 tensor(6.4968e-07, grad_fn=<MseLossBackward>)\n",
      "872 tensor(6.4025e-07, grad_fn=<MseLossBackward>)\n",
      "873 tensor(6.3116e-07, grad_fn=<MseLossBackward>)\n",
      "874 tensor(6.2191e-07, grad_fn=<MseLossBackward>)\n",
      "875 tensor(6.1305e-07, grad_fn=<MseLossBackward>)\n",
      "876 tensor(6.0425e-07, grad_fn=<MseLossBackward>)\n",
      "877 tensor(5.9542e-07, grad_fn=<MseLossBackward>)\n",
      "878 tensor(5.8675e-07, grad_fn=<MseLossBackward>)\n",
      "879 tensor(5.7849e-07, grad_fn=<MseLossBackward>)\n",
      "880 tensor(5.7025e-07, grad_fn=<MseLossBackward>)\n",
      "881 tensor(5.6207e-07, grad_fn=<MseLossBackward>)\n",
      "882 tensor(5.5399e-07, grad_fn=<MseLossBackward>)\n",
      "883 tensor(5.4627e-07, grad_fn=<MseLossBackward>)\n",
      "884 tensor(5.3826e-07, grad_fn=<MseLossBackward>)\n",
      "885 tensor(5.3065e-07, grad_fn=<MseLossBackward>)\n",
      "886 tensor(5.2276e-07, grad_fn=<MseLossBackward>)\n",
      "887 tensor(5.1493e-07, grad_fn=<MseLossBackward>)\n",
      "888 tensor(5.0811e-07, grad_fn=<MseLossBackward>)\n",
      "889 tensor(5.0039e-07, grad_fn=<MseLossBackward>)\n",
      "890 tensor(4.9334e-07, grad_fn=<MseLossBackward>)\n",
      "891 tensor(4.8634e-07, grad_fn=<MseLossBackward>)\n",
      "892 tensor(4.7907e-07, grad_fn=<MseLossBackward>)\n",
      "893 tensor(4.7249e-07, grad_fn=<MseLossBackward>)\n",
      "894 tensor(4.6560e-07, grad_fn=<MseLossBackward>)\n",
      "895 tensor(4.5892e-07, grad_fn=<MseLossBackward>)\n",
      "896 tensor(4.5205e-07, grad_fn=<MseLossBackward>)\n",
      "897 tensor(4.4594e-07, grad_fn=<MseLossBackward>)\n",
      "898 tensor(4.3967e-07, grad_fn=<MseLossBackward>)\n",
      "899 tensor(4.3302e-07, grad_fn=<MseLossBackward>)\n",
      "900 tensor(4.2673e-07, grad_fn=<MseLossBackward>)\n",
      "901 tensor(4.2079e-07, grad_fn=<MseLossBackward>)\n",
      "902 tensor(4.1459e-07, grad_fn=<MseLossBackward>)\n",
      "903 tensor(4.0873e-07, grad_fn=<MseLossBackward>)\n",
      "904 tensor(4.0299e-07, grad_fn=<MseLossBackward>)\n",
      "905 tensor(3.9722e-07, grad_fn=<MseLossBackward>)\n",
      "906 tensor(3.9116e-07, grad_fn=<MseLossBackward>)\n",
      "907 tensor(3.8572e-07, grad_fn=<MseLossBackward>)\n",
      "908 tensor(3.8043e-07, grad_fn=<MseLossBackward>)\n",
      "909 tensor(3.7478e-07, grad_fn=<MseLossBackward>)\n",
      "910 tensor(3.6918e-07, grad_fn=<MseLossBackward>)\n",
      "911 tensor(3.6425e-07, grad_fn=<MseLossBackward>)\n",
      "912 tensor(3.5873e-07, grad_fn=<MseLossBackward>)\n",
      "913 tensor(3.5387e-07, grad_fn=<MseLossBackward>)\n",
      "914 tensor(3.4870e-07, grad_fn=<MseLossBackward>)\n",
      "915 tensor(3.4353e-07, grad_fn=<MseLossBackward>)\n",
      "916 tensor(3.3878e-07, grad_fn=<MseLossBackward>)\n",
      "917 tensor(3.3369e-07, grad_fn=<MseLossBackward>)\n",
      "918 tensor(3.2900e-07, grad_fn=<MseLossBackward>)\n",
      "919 tensor(3.2425e-07, grad_fn=<MseLossBackward>)\n",
      "920 tensor(3.1953e-07, grad_fn=<MseLossBackward>)\n",
      "921 tensor(3.1518e-07, grad_fn=<MseLossBackward>)\n",
      "922 tensor(3.1049e-07, grad_fn=<MseLossBackward>)\n",
      "923 tensor(3.0613e-07, grad_fn=<MseLossBackward>)\n",
      "924 tensor(3.0161e-07, grad_fn=<MseLossBackward>)\n",
      "925 tensor(2.9738e-07, grad_fn=<MseLossBackward>)\n",
      "926 tensor(2.9312e-07, grad_fn=<MseLossBackward>)\n",
      "927 tensor(2.8860e-07, grad_fn=<MseLossBackward>)\n",
      "928 tensor(2.8468e-07, grad_fn=<MseLossBackward>)\n",
      "929 tensor(2.8057e-07, grad_fn=<MseLossBackward>)\n",
      "930 tensor(2.7639e-07, grad_fn=<MseLossBackward>)\n",
      "931 tensor(2.7250e-07, grad_fn=<MseLossBackward>)\n",
      "932 tensor(2.6868e-07, grad_fn=<MseLossBackward>)\n",
      "933 tensor(2.6460e-07, grad_fn=<MseLossBackward>)\n",
      "934 tensor(2.6108e-07, grad_fn=<MseLossBackward>)\n",
      "935 tensor(2.5735e-07, grad_fn=<MseLossBackward>)\n",
      "936 tensor(2.5342e-07, grad_fn=<MseLossBackward>)\n",
      "937 tensor(2.4989e-07, grad_fn=<MseLossBackward>)\n",
      "938 tensor(2.4624e-07, grad_fn=<MseLossBackward>)\n",
      "939 tensor(2.4276e-07, grad_fn=<MseLossBackward>)\n",
      "940 tensor(2.3916e-07, grad_fn=<MseLossBackward>)\n",
      "941 tensor(2.3593e-07, grad_fn=<MseLossBackward>)\n",
      "942 tensor(2.3258e-07, grad_fn=<MseLossBackward>)\n",
      "943 tensor(2.2906e-07, grad_fn=<MseLossBackward>)\n",
      "944 tensor(2.2598e-07, grad_fn=<MseLossBackward>)\n",
      "945 tensor(2.2243e-07, grad_fn=<MseLossBackward>)\n",
      "946 tensor(2.1940e-07, grad_fn=<MseLossBackward>)\n",
      "947 tensor(2.1617e-07, grad_fn=<MseLossBackward>)\n",
      "948 tensor(2.1310e-07, grad_fn=<MseLossBackward>)\n",
      "949 tensor(2.1013e-07, grad_fn=<MseLossBackward>)\n",
      "950 tensor(2.0694e-07, grad_fn=<MseLossBackward>)\n",
      "951 tensor(2.0423e-07, grad_fn=<MseLossBackward>)\n",
      "952 tensor(2.0103e-07, grad_fn=<MseLossBackward>)\n",
      "953 tensor(1.9836e-07, grad_fn=<MseLossBackward>)\n",
      "954 tensor(1.9547e-07, grad_fn=<MseLossBackward>)\n",
      "955 tensor(1.9255e-07, grad_fn=<MseLossBackward>)\n",
      "956 tensor(1.8970e-07, grad_fn=<MseLossBackward>)\n",
      "957 tensor(1.8708e-07, grad_fn=<MseLossBackward>)\n",
      "958 tensor(1.8427e-07, grad_fn=<MseLossBackward>)\n",
      "959 tensor(1.8168e-07, grad_fn=<MseLossBackward>)\n",
      "960 tensor(1.7931e-07, grad_fn=<MseLossBackward>)\n",
      "961 tensor(1.7669e-07, grad_fn=<MseLossBackward>)\n",
      "962 tensor(1.7396e-07, grad_fn=<MseLossBackward>)\n",
      "963 tensor(1.7164e-07, grad_fn=<MseLossBackward>)\n",
      "964 tensor(1.6914e-07, grad_fn=<MseLossBackward>)\n",
      "965 tensor(1.6664e-07, grad_fn=<MseLossBackward>)\n",
      "966 tensor(1.6418e-07, grad_fn=<MseLossBackward>)\n",
      "967 tensor(1.6186e-07, grad_fn=<MseLossBackward>)\n",
      "968 tensor(1.5941e-07, grad_fn=<MseLossBackward>)\n",
      "969 tensor(1.5717e-07, grad_fn=<MseLossBackward>)\n",
      "970 tensor(1.5494e-07, grad_fn=<MseLossBackward>)\n",
      "971 tensor(1.5273e-07, grad_fn=<MseLossBackward>)\n",
      "972 tensor(1.5053e-07, grad_fn=<MseLossBackward>)\n",
      "973 tensor(1.4835e-07, grad_fn=<MseLossBackward>)\n",
      "974 tensor(1.4621e-07, grad_fn=<MseLossBackward>)\n",
      "975 tensor(1.4421e-07, grad_fn=<MseLossBackward>)\n",
      "976 tensor(1.4226e-07, grad_fn=<MseLossBackward>)\n",
      "977 tensor(1.4025e-07, grad_fn=<MseLossBackward>)\n",
      "978 tensor(1.3818e-07, grad_fn=<MseLossBackward>)\n",
      "979 tensor(1.3610e-07, grad_fn=<MseLossBackward>)\n",
      "980 tensor(1.3417e-07, grad_fn=<MseLossBackward>)\n",
      "981 tensor(1.3228e-07, grad_fn=<MseLossBackward>)\n",
      "982 tensor(1.3022e-07, grad_fn=<MseLossBackward>)\n",
      "983 tensor(1.2836e-07, grad_fn=<MseLossBackward>)\n",
      "984 tensor(1.2676e-07, grad_fn=<MseLossBackward>)\n",
      "985 tensor(1.2480e-07, grad_fn=<MseLossBackward>)\n",
      "986 tensor(1.2312e-07, grad_fn=<MseLossBackward>)\n",
      "987 tensor(1.2109e-07, grad_fn=<MseLossBackward>)\n",
      "988 tensor(1.1944e-07, grad_fn=<MseLossBackward>)\n",
      "989 tensor(1.1783e-07, grad_fn=<MseLossBackward>)\n",
      "990 tensor(1.1620e-07, grad_fn=<MseLossBackward>)\n",
      "991 tensor(1.1442e-07, grad_fn=<MseLossBackward>)\n",
      "992 tensor(1.1282e-07, grad_fn=<MseLossBackward>)\n",
      "993 tensor(1.1105e-07, grad_fn=<MseLossBackward>)\n",
      "994 tensor(1.0946e-07, grad_fn=<MseLossBackward>)\n",
      "995 tensor(1.0805e-07, grad_fn=<MseLossBackward>)\n",
      "996 tensor(1.0631e-07, grad_fn=<MseLossBackward>)\n",
      "997 tensor(1.0490e-07, grad_fn=<MseLossBackward>)\n",
      "998 tensor(1.0336e-07, grad_fn=<MseLossBackward>)\n",
      "999 tensor(1.0187e-07, grad_fn=<MseLossBackward>)\n",
      "w= 1.999787449836731\n",
      "b= 0.0004831004189327359\n",
      "tensor([[7.9996]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_data=torch.Tensor([[1.0],[2.0],[3.0]])\n",
    "y_data=torch.Tensor([[2.0],[4.0],[6.0]])\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel,self).__init__()\n",
    "        self.linear=torch.nn.Linear(1,1)#构造一个对象，包含权重与偏置\n",
    "                                        #参数1：输入维度，参数2：输出维度，参数3：bias 默认为True\n",
    "    def forward(self,x):\n",
    "        y_pred=self.linear(x)\n",
    "        return y_pred\n",
    "model=LinearModel()\n",
    "criterion=torch.nn.MSELoss(size_average=False)#损失值  参数1：是否求均值，参数2：是否求和（将维度降下来）\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)#优化器 ：参数1：对哪些参数进行优化，参数2：学习步长\n",
    "for epoch in range(1000):\n",
    "    y_pred=model.forward(x_data)\n",
    "    loss=criterion(y_pred,y_data)\n",
    "    print(epoch,loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('w=',model.linear.weight.item())\n",
    "print('b=',model.linear.bias.item())\n",
    "x_test=torch.Tensor([[4.0]])\n",
    "y_test=model.forward(x_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 0 ====================\n",
      "outputs size: torch.Size([1, 2])\n",
      "tensor([[-0.7108, -0.2346]], grad_fn=<TanhBackward>)\n",
      "==================== 1 ====================\n",
      "outputs size: torch.Size([1, 2])\n",
      "tensor([[-0.4331, -0.6294]], grad_fn=<TanhBackward>)\n",
      "==================== 2 ====================\n",
      "outputs size: torch.Size([1, 2])\n",
      "tensor([[ 0.7286, -0.9450]], grad_fn=<TanhBackward>)\n",
      "Predicted string: h l l e l ,Epoch [1/15] loss=7.5183\n",
      "Predicted string: h l l l l ,Epoch [2/15] loss=6.2722\n",
      "Predicted string: h h l l l ,Epoch [3/15] loss=5.4352\n",
      "Predicted string: h h l o l ,Epoch [4/15] loss=4.8921\n",
      "Predicted string: o h l o o ,Epoch [5/15] loss=4.6255\n",
      "Predicted string: o h l o o ,Epoch [6/15] loss=4.4360\n",
      "Predicted string: o h l o l ,Epoch [7/15] loss=4.2753\n",
      "Predicted string: o h l o l ,Epoch [8/15] loss=4.1282\n",
      "Predicted string: o h l l l ,Epoch [9/15] loss=3.9774\n",
      "Predicted string: o h l l l ,Epoch [10/15] loss=3.8201\n",
      "Predicted string: o h l l l ,Epoch [11/15] loss=3.6622\n",
      "Predicted string: o h l l l ,Epoch [12/15] loss=3.4930\n",
      "Predicted string: o h l l l ,Epoch [13/15] loss=3.3024\n",
      "Predicted string: o h l l l ,Epoch [14/15] loss=3.1079\n",
      "Predicted string: o h l l l ,Epoch [15/15] loss=2.9191\n"
     ]
    }
   ],
   "source": [
    "batchSize=1 #序列的个数\n",
    "seqLen=3 #序列长度\n",
    "inputSize=4 #每个元素具有的维度\n",
    "hiddenSize=2 #输出维度\n",
    "\n",
    "'''\n",
    "input.shape=(batchSize,inputSize)\n",
    "output.shape=(batchSize,hiddenSize)#h0\n",
    "\n",
    "dataset.shape=(seqLen,batchSize,inputSize)\n",
    "'''\n",
    "batch_size=1\n",
    "seq_len=3\n",
    "input_size=4\n",
    "hidden_size=2\n",
    "\n",
    "cell=torch.nn.RNNCell(input_size=input_size,hidden_size=hidden_size)\n",
    "dataset=torch.randn(seq_len,batch_size,input_size)\n",
    "hidden=torch.zeros(batch_size,hidden_size)\n",
    "\n",
    "for idx,input in enumerate(dataset):\n",
    "    print('='*20,idx,'='*20)\n",
    "    hidden=cell(input,hidden)\n",
    "    \n",
    "    print('outputs size:',hidden.shape)\n",
    "    print(hidden)\n",
    "batch_size=1\n",
    "seq_len=3\n",
    "input_size=4\n",
    "hidden_size=2\n",
    "num_layers=1\n",
    "\n",
    "cell=torch.nn.RNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)\n",
    "inputs=torch.randn(seq_len,batch_size,input_size)\n",
    "hidden=torch.zeros(num_layers,batch_size,hidden_size)\n",
    "\n",
    "out,hidden=cell(inputs,hidden) #out为每次循环的h，hidden为最后一个循环的h\n",
    "#inputs的维度：seq_len，batch，input_size\n",
    "#hidden的维度：numlayers，batch，hidden_size\n",
    "#out的维度：seqlen，batch，hidden——size\n",
    "#hidden 的维度：与前面的hidden相同\n",
    "input_size=4\n",
    "hidden_size=4\n",
    "batch_size=1\n",
    "idx2char=['e','h','l','o']\n",
    "x_data=[1,0,2,2,3]#hello#(5*1)5为seqlen\n",
    "y_data=[3,1,2,3,2]#ohlol\n",
    "one_hot_lookup=[[1,0,0,0],\n",
    "               [0,1,0,0],\n",
    "               [0,0,1,0],\n",
    "               [0,0,0,1]]\n",
    "x_one_hot=[one_hot_lookup[x] for x in x_data]\n",
    "x_one_hot#(5*4)#4为input_size\n",
    "inputs=torch.Tensor(x_one_hot).view(-1,batch_size,input_size)#view相当于reshape,(5*1*4)\n",
    "labels=torch.LongTensor(y_data).view(-1,1)\n",
    "input_size=4\n",
    "hidden_size=4\n",
    "batch_size=1\n",
    "idx2char=['e','h','l','o']\n",
    "x_data=[1,0,2,2,3]#hello\n",
    "y_data=[3,1,2,3,2]#ohlol\n",
    "one_hot_lookup=[[1,0,0,0],\n",
    "               [0,1,0,0],\n",
    "               [0,0,1,0],\n",
    "               [0,0,0,1]]\n",
    "x_one_hot=[one_hot_lookup[x] for x in x_data]\n",
    "\n",
    "inputs=torch.Tensor(x_one_hot).view(-1,batch_size,input_size)#view相当于reshape\n",
    "labels=torch.LongTensor(y_data).view(-1,1)#(seqlen,batch)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,batch_size,input_size,hidden_size):\n",
    "        super(Model,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.rnncell=torch.nn.RNNCell(input_size=self.input_size,hidden_size=self.hidden_size)\n",
    "        \n",
    "    def forward(self,input,hidden):\n",
    "        hidden=self.rnncell(input,hidden)\n",
    "        return hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.batch_size,self.hidden_size)\n",
    "    \n",
    "net=Model(batch_size,input_size,hidden_size)\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.1)\n",
    "for epoch in range(15):\n",
    "    loss=0\n",
    "    optimizer.zero_grad()\n",
    "    hidden=net.init_hidden()\n",
    "    print('Predicted string:',end=' ')\n",
    "    for input,label in zip(inputs,labels):\n",
    "        hidden=net.forward(input,hidden)\n",
    "        loss+=criterion(hidden,label)\n",
    "        _,idx=hidden.max(dim=1)\n",
    "        print(idx2char[idx.item()],end=' ')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(',Epoch [%d/15] loss=%.4f'%(epoch+1,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ellll,Epoch [1/15] loss=1.4182\n",
      "Predicted: helll,Epoch [2/15] loss=1.1558\n",
      "Predicted: hhlol,Epoch [3/15] loss=1.0006\n",
      "Predicted: ohlol,Epoch [4/15] loss=0.8906\n",
      "Predicted: ohlol,Epoch [5/15] loss=0.8358\n",
      "Predicted: ohlll,Epoch [6/15] loss=0.8042\n",
      "Predicted: ohlll,Epoch [7/15] loss=0.7645\n",
      "Predicted: ohlll,Epoch [8/15] loss=0.7228\n",
      "Predicted: ohlll,Epoch [9/15] loss=0.6883\n",
      "Predicted: ohlll,Epoch [10/15] loss=0.6570\n",
      "Predicted: ohlll,Epoch [11/15] loss=0.6250\n",
      "Predicted: ohlll,Epoch [12/15] loss=0.5925\n",
      "Predicted: ohlll,Epoch [13/15] loss=0.5614\n",
      "Predicted: ohlll,Epoch [14/15] loss=0.5338\n",
      "Predicted: ohlll,Epoch [15/15] loss=0.5121\n"
     ]
    }
   ],
   "source": [
    "num_layers=1\n",
    "input_size=4\n",
    "hidden_size=4\n",
    "batch_size=1\n",
    "idx2char=['e','h','l','o']\n",
    "x_data=[1,0,2,2,3]#hello\n",
    "y_data=[3,1,2,3,2]#ohlol\n",
    "one_hot_lookup=[[1,0,0,0],\n",
    "               [0,1,0,0],\n",
    "               [0,0,1,0],\n",
    "               [0,0,0,1]]\n",
    "x_one_hot=[one_hot_lookup[x] for x in x_data]\n",
    "\n",
    "inputs=torch.Tensor(x_one_hot).view(-1,batch_size,input_size)#view相当于reshape\n",
    "labels=torch.LongTensor(y_data)#(seqlen*batch*1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,batch_size,input_size,hidden_size,num_layers=1):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        self.num_layers=num_layers\n",
    "        self.batch_size=batch_size\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.rnn=torch.nn.RNN(input_size=self.input_size,hidden_size=self.hidden_size,num_layers=self.num_layers)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        hidden=torch.zeros(self.num_layers,self.batch_size,self.hidden_size)\n",
    "        \n",
    "        out,_=self.rnn(input,hidden)\n",
    "        return out.view(-1,self.hidden_size)\n",
    "net=Model(batch_size,input_size,hidden_size,num_layers)\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs=net.forward(inputs)\n",
    "    loss=criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    _,idx=outputs.max(dim=1)\n",
    "    idx=idx.data.numpy()\n",
    "    print('Predicted:',''.join([idx2char[x] for x in idx]),end='')\n",
    "    print(',Epoch [%d/15] loss=%.4f'%(epoch+1,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
